{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: POS tagging, Sequence labelling, RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK4tXYIZPWgv"
      },
      "source": [
        "\n",
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "* Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5DpJrkwPWgv"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "You are tasked to address the task of POS tagging.\n",
        "\n",
        "<center>\n",
        "    <img src=\"images/pos_tagging.png\" alt=\"POS tagging\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import zeros\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.vocab import GloVe\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import numpy as np\n",
        "from sys import stdout\n",
        "from typing import Tuple\n",
        "flush, write = stdout.flush , stdout.write"
      ],
      "metadata": {
        "id": "YqCsaSYMRAN1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UV8pgMPWgw"
      },
      "source": [
        "# [Task 1 - 0.5 points] Corpus\n",
        "\n",
        "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
        "\n",
        "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
        "\n",
        "### Example\n",
        "\n",
        "```Pierre\tNNP\t2\n",
        "Vinken\tNNP\t8\n",
        ",\t,\t2\n",
        "61\tCD\t5\n",
        "years\tNNS\t6\n",
        "old\tJJ\t2\n",
        ",\t,\t2\n",
        "will\tMD\t0\n",
        "join\tVB\t8\n",
        "the\tDT\t11\n",
        "board\tNN\t9\n",
        "as\tIN\t9\n",
        "a\tDT\t15\n",
        "nonexecutive\tJJ\t15\n",
        "director\tNN\t12\n",
        "Nov.\tNNP\t9\n",
        "29\tCD\t16\n",
        ".\t.\t8\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset download"
      ],
      "metadata": {
        "id": "Evf0B3k4Q20o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "!unzip -o dependency_treebank.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAoEoejnQVc8",
        "outputId": "2907dc65-5e26-48a3-c1da-d0d46749857f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  446k  100  446k    0     0  1050k      0 --:--:-- --:--:-- --:--:-- 1051k\n",
            "Archive:  dependency_treebank.zip\n",
            "   creating: dependency_treebank/\n",
            "  inflating: dependency_treebank/wsj_0093.dp  \n",
            "  inflating: dependency_treebank/wsj_0065.dp  \n",
            "  inflating: dependency_treebank/wsj_0039.dp  \n",
            "  inflating: dependency_treebank/wsj_0182.dp  \n",
            "  inflating: dependency_treebank/wsj_0186.dp  \n",
            "  inflating: dependency_treebank/wsj_0041.dp  \n",
            "  inflating: dependency_treebank/wsj_0018.dp  \n",
            "  inflating: dependency_treebank/wsj_0105.dp  \n",
            "  inflating: dependency_treebank/wsj_0149.dp  \n",
            "  inflating: dependency_treebank/wsj_0194.dp  \n",
            "  inflating: dependency_treebank/wsj_0055.dp  \n",
            "  inflating: dependency_treebank/wsj_0187.dp  \n",
            "  inflating: dependency_treebank/wsj_0143.dp  \n",
            "  inflating: dependency_treebank/wsj_0052.dp  \n",
            "  inflating: dependency_treebank/wsj_0064.dp  \n",
            "  inflating: dependency_treebank/wsj_0179.dp  \n",
            "  inflating: dependency_treebank/wsj_0195.dp  \n",
            "  inflating: dependency_treebank/wsj_0051.dp  \n",
            "  inflating: dependency_treebank/wsj_0059.dp  \n",
            "  inflating: dependency_treebank/wsj_0109.dp  \n",
            "  inflating: dependency_treebank/wsj_0074.dp  \n",
            "  inflating: dependency_treebank/wsj_0089.dp  \n",
            "  inflating: dependency_treebank/wsj_0108.dp  \n",
            "  inflating: dependency_treebank/wsj_0104.dp  \n",
            "  inflating: dependency_treebank/wsj_0164.dp  \n",
            "  inflating: dependency_treebank/wsj_0024.dp  \n",
            "  inflating: dependency_treebank/wsj_0008.dp  \n",
            "  inflating: dependency_treebank/wsj_0101.dp  \n",
            "  inflating: dependency_treebank/wsj_0132.dp  \n",
            "  inflating: dependency_treebank/wsj_0028.dp  \n",
            "  inflating: dependency_treebank/wsj_0184.dp  \n",
            "  inflating: dependency_treebank/wsj_0082.dp  \n",
            "  inflating: dependency_treebank/wsj_0114.dp  \n",
            "  inflating: dependency_treebank/wsj_0061.dp  \n",
            "  inflating: dependency_treebank/wsj_0190.dp  \n",
            "  inflating: dependency_treebank/wsj_0034.dp  \n",
            "  inflating: dependency_treebank/wsj_0043.dp  \n",
            "  inflating: dependency_treebank/wsj_0044.dp  \n",
            "  inflating: dependency_treebank/wsj_0021.dp  \n",
            "  inflating: dependency_treebank/wsj_0005.dp  \n",
            "  inflating: dependency_treebank/wsj_0112.dp  \n",
            "  inflating: dependency_treebank/wsj_0167.dp  \n",
            "  inflating: dependency_treebank/wsj_0042.dp  \n",
            "  inflating: dependency_treebank/wsj_0168.dp  \n",
            "  inflating: dependency_treebank/wsj_0185.dp  \n",
            "  inflating: dependency_treebank/wsj_0057.dp  \n",
            "  inflating: dependency_treebank/wsj_0015.dp  \n",
            "  inflating: dependency_treebank/wsj_0116.dp  \n",
            "  inflating: dependency_treebank/wsj_0135.dp  \n",
            "  inflating: dependency_treebank/wsj_0175.dp  \n",
            "  inflating: dependency_treebank/wsj_0171.dp  \n",
            "  inflating: dependency_treebank/wsj_0068.dp  \n",
            "  inflating: dependency_treebank/wsj_0080.dp  \n",
            "  inflating: dependency_treebank/wsj_0035.dp  \n",
            "  inflating: dependency_treebank/wsj_0181.dp  \n",
            "  inflating: dependency_treebank/wsj_0177.dp  \n",
            "  inflating: dependency_treebank/wsj_0102.dp  \n",
            "  inflating: dependency_treebank/wsj_0137.dp  \n",
            "  inflating: dependency_treebank/wsj_0022.dp  \n",
            "  inflating: dependency_treebank/wsj_0176.dp  \n",
            "  inflating: dependency_treebank/wsj_0180.dp  \n",
            "  inflating: dependency_treebank/wsj_0121.dp  \n",
            "  inflating: dependency_treebank/wsj_0128.dp  \n",
            "  inflating: dependency_treebank/wsj_0036.dp  \n",
            "  inflating: dependency_treebank/wsj_0071.dp  \n",
            "  inflating: dependency_treebank/wsj_0091.dp  \n",
            "  inflating: dependency_treebank/wsj_0076.dp  \n",
            "  inflating: dependency_treebank/wsj_0123.dp  \n",
            "  inflating: dependency_treebank/wsj_0075.dp  \n",
            "  inflating: dependency_treebank/wsj_0131.dp  \n",
            "  inflating: dependency_treebank/wsj_0050.dp  \n",
            "  inflating: dependency_treebank/wsj_0136.dp  \n",
            "  inflating: dependency_treebank/wsj_0161.dp  \n",
            "  inflating: dependency_treebank/wsj_0033.dp  \n",
            "  inflating: dependency_treebank/wsj_0188.dp  \n",
            "  inflating: dependency_treebank/wsj_0085.dp  \n",
            "  inflating: dependency_treebank/wsj_0014.dp  \n",
            "  inflating: dependency_treebank/wsj_0073.dp  \n",
            "  inflating: dependency_treebank/wsj_0199.dp  \n",
            "  inflating: dependency_treebank/wsj_0120.dp  \n",
            "  inflating: dependency_treebank/wsj_0178.dp  \n",
            "  inflating: dependency_treebank/wsj_0122.dp  \n",
            "  inflating: dependency_treebank/wsj_0040.dp  \n",
            "  inflating: dependency_treebank/wsj_0020.dp  \n",
            "  inflating: dependency_treebank/wsj_0153.dp  \n",
            "  inflating: dependency_treebank/wsj_0107.dp  \n",
            "  inflating: dependency_treebank/wsj_0017.dp  \n",
            "  inflating: dependency_treebank/wsj_0140.dp  \n",
            "  inflating: dependency_treebank/wsj_0038.dp  \n",
            "  inflating: dependency_treebank/wsj_0031.dp  \n",
            "  inflating: dependency_treebank/wsj_0165.dp  \n",
            "  inflating: dependency_treebank/wsj_0146.dp  \n",
            "  inflating: dependency_treebank/wsj_0090.dp  \n",
            "  inflating: dependency_treebank/wsj_0001.dp  \n",
            "  inflating: dependency_treebank/wsj_0148.dp  \n",
            "  inflating: dependency_treebank/wsj_0097.dp  \n",
            "  inflating: dependency_treebank/wsj_0009.dp  \n",
            "  inflating: dependency_treebank/wsj_0173.dp  \n",
            "  inflating: dependency_treebank/wsj_0111.dp  \n",
            "  inflating: dependency_treebank/wsj_0129.dp  \n",
            "  inflating: dependency_treebank/wsj_0130.dp  \n",
            "  inflating: dependency_treebank/wsj_0047.dp  \n",
            "  inflating: dependency_treebank/wsj_0110.dp  \n",
            "  inflating: dependency_treebank/wsj_0113.dp  \n",
            "  inflating: dependency_treebank/wsj_0147.dp  \n",
            "  inflating: dependency_treebank/wsj_0160.dp  \n",
            "  inflating: dependency_treebank/wsj_0099.dp  \n",
            "  inflating: dependency_treebank/wsj_0003.dp  \n",
            "  inflating: dependency_treebank/wsj_0011.dp  \n",
            "  inflating: dependency_treebank/wsj_0056.dp  \n",
            "  inflating: dependency_treebank/wsj_0069.dp  \n",
            "  inflating: dependency_treebank/wsj_0026.dp  \n",
            "  inflating: dependency_treebank/wsj_0138.dp  \n",
            "  inflating: dependency_treebank/wsj_0029.dp  \n",
            "  inflating: dependency_treebank/wsj_0115.dp  \n",
            "  inflating: dependency_treebank/wsj_0037.dp  \n",
            "  inflating: dependency_treebank/wsj_0019.dp  \n",
            "  inflating: dependency_treebank/wsj_0002.dp  \n",
            "  inflating: dependency_treebank/wsj_0007.dp  \n",
            "  inflating: dependency_treebank/wsj_0158.dp  \n",
            "  inflating: dependency_treebank/wsj_0087.dp  \n",
            "  inflating: dependency_treebank/wsj_0157.dp  \n",
            "  inflating: dependency_treebank/wsj_0083.dp  \n",
            "  inflating: dependency_treebank/wsj_0103.dp  \n",
            "  inflating: dependency_treebank/wsj_0058.dp  \n",
            "  inflating: dependency_treebank/wsj_0054.dp  \n",
            "  inflating: dependency_treebank/wsj_0016.dp  \n",
            "  inflating: dependency_treebank/wsj_0126.dp  \n",
            "  inflating: dependency_treebank/wsj_0198.dp  \n",
            "  inflating: dependency_treebank/wsj_0144.dp  \n",
            "  inflating: dependency_treebank/wsj_0096.dp  \n",
            "  inflating: dependency_treebank/wsj_0086.dp  \n",
            "  inflating: dependency_treebank/wsj_0197.dp  \n",
            "  inflating: dependency_treebank/wsj_0025.dp  \n",
            "  inflating: dependency_treebank/wsj_0100.dp  \n",
            "  inflating: dependency_treebank/wsj_0084.dp  \n",
            "  inflating: dependency_treebank/wsj_0098.dp  \n",
            "  inflating: dependency_treebank/wsj_0106.dp  \n",
            "  inflating: dependency_treebank/wsj_0119.dp  \n",
            "  inflating: dependency_treebank/wsj_0092.dp  \n",
            "  inflating: dependency_treebank/wsj_0134.dp  \n",
            "  inflating: dependency_treebank/wsj_0077.dp  \n",
            "  inflating: dependency_treebank/wsj_0060.dp  \n",
            "  inflating: dependency_treebank/wsj_0172.dp  \n",
            "  inflating: dependency_treebank/wsj_0048.dp  \n",
            "  inflating: dependency_treebank/wsj_0030.dp  \n",
            "  inflating: dependency_treebank/wsj_0192.dp  \n",
            "  inflating: dependency_treebank/wsj_0066.dp  \n",
            "  inflating: dependency_treebank/wsj_0045.dp  \n",
            "  inflating: dependency_treebank/wsj_0155.dp  \n",
            "  inflating: dependency_treebank/wsj_0118.dp  \n",
            "  inflating: dependency_treebank/wsj_0152.dp  \n",
            "  inflating: dependency_treebank/wsj_0012.dp  \n",
            "  inflating: dependency_treebank/wsj_0006.dp  \n",
            "  inflating: dependency_treebank/wsj_0159.dp  \n",
            "  inflating: dependency_treebank/wsj_0163.dp  \n",
            "  inflating: dependency_treebank/wsj_0170.dp  \n",
            "  inflating: dependency_treebank/wsj_0141.dp  \n",
            "  inflating: dependency_treebank/wsj_0117.dp  \n",
            "  inflating: dependency_treebank/wsj_0125.dp  \n",
            "  inflating: dependency_treebank/wsj_0094.dp  \n",
            "  inflating: dependency_treebank/wsj_0169.dp  \n",
            "  inflating: dependency_treebank/wsj_0027.dp  \n",
            "  inflating: dependency_treebank/wsj_0010.dp  \n",
            "  inflating: dependency_treebank/wsj_0162.dp  \n",
            "  inflating: dependency_treebank/wsj_0127.dp  \n",
            "  inflating: dependency_treebank/wsj_0142.dp  \n",
            "  inflating: dependency_treebank/wsj_0046.dp  \n",
            "  inflating: dependency_treebank/wsj_0088.dp  \n",
            "  inflating: dependency_treebank/wsj_0079.dp  \n",
            "  inflating: dependency_treebank/wsj_0174.dp  \n",
            "  inflating: dependency_treebank/wsj_0063.dp  \n",
            "  inflating: dependency_treebank/wsj_0023.dp  \n",
            "  inflating: dependency_treebank/wsj_0004.dp  \n",
            "  inflating: dependency_treebank/wsj_0156.dp  \n",
            "  inflating: dependency_treebank/wsj_0133.dp  \n",
            "  inflating: dependency_treebank/wsj_0032.dp  \n",
            "  inflating: dependency_treebank/wsj_0070.dp  \n",
            "  inflating: dependency_treebank/wsj_0154.dp  \n",
            "  inflating: dependency_treebank/wsj_0095.dp  \n",
            "  inflating: dependency_treebank/wsj_0072.dp  \n",
            "  inflating: dependency_treebank/wsj_0183.dp  \n",
            "  inflating: dependency_treebank/wsj_0081.dp  \n",
            "  inflating: dependency_treebank/wsj_0196.dp  \n",
            "  inflating: dependency_treebank/wsj_0062.dp  \n",
            "  inflating: dependency_treebank/wsj_0124.dp  \n",
            "  inflating: dependency_treebank/wsj_0191.dp  \n",
            "  inflating: dependency_treebank/wsj_0013.dp  \n",
            "  inflating: dependency_treebank/wsj_0078.dp  \n",
            "  inflating: dependency_treebank/wsj_0150.dp  \n",
            "  inflating: dependency_treebank/wsj_0049.dp  \n",
            "  inflating: dependency_treebank/wsj_0189.dp  \n",
            "  inflating: dependency_treebank/wsj_0151.dp  \n",
            "  inflating: dependency_treebank/wsj_0193.dp  \n",
            "  inflating: dependency_treebank/wsj_0067.dp  \n",
            "  inflating: dependency_treebank/wsj_0145.dp  \n",
            "  inflating: dependency_treebank/wsj_0139.dp  \n",
            "  inflating: dependency_treebank/wsj_0166.dp  \n",
            "  inflating: dependency_treebank/wsj_0053.dp  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkhLvBtRPWgw"
      },
      "source": [
        "### Splits\n",
        "\n",
        "The corpus contains 200 documents.\n",
        "\n",
        "   * **Train**: Documents 1-100\n",
        "   * **Validation**: Documents 101-150\n",
        "   * **Test**: Documents 151-199"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset import"
      ],
      "metadata": {
        "id": "MtoJ6bs_RBnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(x, tot_len, pad_val):\n",
        "  l = len(x)\n",
        "  return ''.join([pad_val for _ in range(tot_len - l)]) + x"
      ],
      "metadata": {
        "id": "etSG9VDOTq7G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_corpus(folder:'str', file_base:'str', corpus_range:'range', separator:'str', data:'object'):\n",
        "  keys = list(data.keys())\n",
        "  data_len = len(keys)\n",
        "  data['source file'] = []\n",
        "  for i in corpus_range:\n",
        "    f = open(f'{folder}/{file_base}{pad(str(i), 4, \"0\")}.dp')\n",
        "    for line in f.readlines():\n",
        "      entry = line.replace('\\n','').split(separator)\n",
        "      if len(entry) >= data_len:\n",
        "        for j in range(data_len):\n",
        "            data[keys[j]].append(entry[j])\n",
        "        data['source file'].append(f'{file_base}{pad(str(i), 4, \"0\")}')\n",
        "    f.close()\n",
        "  return data"
      ],
      "metadata": {
        "id": "LCRhF-RDVrOw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEPARATOR = '\\t'\n",
        "folder = 'dependency_treebank'\n",
        "file_base = 'wsj_'\n",
        "\n",
        "train_data = pd.DataFrame(import_corpus(folder, file_base, range(1,101), SEPARATOR, {'word/symbol':[], 'pos label':[]}))\n",
        "validation_data = pd.DataFrame(import_corpus(folder, file_base, range(101,151), SEPARATOR, {'word/symbol':[], 'pos label':[]}))\n",
        "test_data = pd.DataFrame(import_corpus(folder, file_base, range(151,200), SEPARATOR, {'word/symbol':[], 'pos label':[]}))\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8nnJIIQITbFq",
        "outputId": "13b2aba3-6312-4135-cf49-e0620d846ed4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      word/symbol pos label source file\n",
              "0          Pierre       NNP    wsj_0001\n",
              "1          Vinken       NNP    wsj_0001\n",
              "2               ,         ,    wsj_0001\n",
              "3              61        CD    wsj_0001\n",
              "4           years       NNS    wsj_0001\n",
              "...           ...       ...         ...\n",
              "47351   challenge        NN    wsj_0100\n",
              "47352          he       PRP    wsj_0100\n",
              "47353         has       VBZ    wsj_0100\n",
              "47354       faced       VBN    wsj_0100\n",
              "47355           .         .    wsj_0100\n",
              "\n",
              "[47356 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d20e2e2-8d8e-405f-844c-7cb1d3dd4a7f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word/symbol</th>\n",
              "      <th>pos label</th>\n",
              "      <th>source file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pierre</td>\n",
              "      <td>NNP</td>\n",
              "      <td>wsj_0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vinken</td>\n",
              "      <td>NNP</td>\n",
              "      <td>wsj_0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>wsj_0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61</td>\n",
              "      <td>CD</td>\n",
              "      <td>wsj_0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>years</td>\n",
              "      <td>NNS</td>\n",
              "      <td>wsj_0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47351</th>\n",
              "      <td>challenge</td>\n",
              "      <td>NN</td>\n",
              "      <td>wsj_0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47352</th>\n",
              "      <td>he</td>\n",
              "      <td>PRP</td>\n",
              "      <td>wsj_0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47353</th>\n",
              "      <td>has</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>wsj_0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47354</th>\n",
              "      <td>faced</td>\n",
              "      <td>VBN</td>\n",
              "      <td>wsj_0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47355</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>wsj_0100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47356 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d20e2e2-8d8e-405f-844c-7cb1d3dd4a7f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d20e2e2-8d8e-405f-844c-7cb1d3dd4a7f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d20e2e2-8d8e-405f-844c-7cb1d3dd4a7f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-561c8222-76b4-4b37-aa13-7d556bb7f2e8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-561c8222-76b4-4b37-aa13-7d556bb7f2e8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-561c8222-76b4-4b37-aa13-7d556bb7f2e8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveiIg-UPWgx"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Download** the corpus.\n",
        "* **Encode** the corpus into a pandas.DataFrame object.\n",
        "* **Split** it in training, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhiw3MBkPWgx"
      },
      "source": [
        "# [Task 2 - 0.5 points] Text encoding\n",
        "\n",
        "To train a neural POS tagger, you first need to encode text into numerical format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuqriwPkPWgx"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove = GloVe(name='6B', dim=100)"
      ],
      "metadata": {
        "id": "Zv4zgrWuTvUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a290ccc-d888-46d9-f010-2e2055cc3f6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:51, 5.03MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:20<00:00, 19357.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 50\n",
        "word = train_data.iloc[i][\"word/symbol\"]\n",
        "print(f\"word at position {i}: {word}\")\n",
        "print(f'embedding: \\n {glove[word]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KFWwF1YUKkG",
        "outputId": "655c89a5-77c3-4d27-c25b-35b3bd7297f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word at position 50: director\n",
            "embedding: \n",
            " tensor([ 2.6554e-01, -7.8286e-01, -8.8447e-02, -1.0131e+00,  9.9533e-01,\n",
            "        -9.4081e-01, -3.6870e-01,  3.3595e-01, -6.6131e-01,  1.8660e-02,\n",
            "        -1.0583e-01, -5.0757e-01,  4.3957e-01,  1.1668e-01,  6.8358e-03,\n",
            "        -1.6246e-01,  8.3221e-01, -2.9842e-02, -5.8107e-01,  5.9797e-01,\n",
            "        -3.4653e-01,  3.3801e-01,  8.3349e-02, -3.3689e-01, -1.5555e-01,\n",
            "         3.0370e-01,  3.6218e-01, -4.9779e-01, -1.0420e-01,  2.3055e-01,\n",
            "        -9.2252e-01,  6.0625e-01, -3.4707e-01,  3.9155e-01, -1.1208e+00,\n",
            "        -5.4766e-02,  9.1888e-02,  1.3057e+00,  1.7112e-01, -4.7524e-01,\n",
            "        -3.8920e-01, -6.1009e-02, -6.0362e-01,  4.8490e-01,  8.2905e-01,\n",
            "         1.9803e-01, -7.3324e-01, -5.5246e-01, -4.6930e-01, -1.7326e-01,\n",
            "         2.5662e-01, -1.0521e+00, -3.2560e-01,  1.3647e-01,  1.2832e-01,\n",
            "        -2.4086e+00, -2.8245e-01,  7.1802e-01,  8.7841e-01,  7.1353e-02,\n",
            "         5.3584e-01,  5.7417e-01,  5.6343e-01, -1.2018e-01,  5.0456e-01,\n",
            "        -6.1081e-01,  4.9771e-01,  1.2290e+00,  7.7136e-01,  1.2948e+00,\n",
            "         7.4913e-01,  1.6564e-01, -2.8906e-01, -5.0207e-01,  3.1034e-01,\n",
            "        -8.0094e-01, -7.7947e-01,  2.1794e-01, -1.0672e+00, -2.2227e-01,\n",
            "         3.4507e-01, -1.3336e-01, -5.8961e-02,  2.1234e-01, -1.4277e+00,\n",
            "         7.4573e-02, -3.1233e-01, -5.3944e-04,  1.5698e-01, -1.0382e+00,\n",
            "         9.8328e-01, -5.9559e-01,  4.3997e-01,  5.5026e-01,  3.6462e-01,\n",
            "         8.4136e-01, -1.4328e-03, -2.4872e-01, -1.0055e-01,  9.5509e-02])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful classes"
      ],
      "metadata": {
        "id": "fF6zaZQl2aDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "it is a usefull class that helps creating a dataloader which is very usefull for training a network since it automatically manages batches"
      ],
      "metadata": {
        "id": "Cpf8ylTl2eMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "SDdt_6ytPtLc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n",
        "It is a class that really helps with managing the words in our dataset. it creates 3 structures:\n",
        "  - `word2idx` which is a dictionary that maps every word to the corresponding token.\n",
        "  - `idx2word` which is a list that works as the inverse function to `word2idx` mapping back every token to the corresponding word.\n",
        "  - `vectors` which is a list that maps every token to the corresponding embedding vector. If no embedding vector has been given for the corresponding word, the vectors list will return a 0 tensor.\n",
        "\n",
        "In addition, the `length` variable will contains the length of the embedding vectors and `dim` will contains the size of the vocabulary."
      ],
      "metadata": {
        "id": "MsNOjTC02-qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  \"\"\"\n",
        "  A class containing all the words used in the training.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  word2idx : Dict\n",
        "    Maps every word to the corresponding token.\n",
        "  idx2word : List[str]\n",
        "    Works as the inverse function to `word2idx` mapping back every token to the corresponding word.\n",
        "  vectors : list[torch.Tensor]\n",
        "    maps every token to the corresponding embedding vector. If no embedding vector has been given for the corresponding word, the vectors list will return a 0 tensor.\n",
        "  length : int\n",
        "    contains the length of the embedding vectors.\n",
        "  dim : int\n",
        "    contains the size of the vocabulary.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               words:'list[str]',\n",
        "               pretrained_vectors: 'torchtext.vocab.Vectors' = None,\n",
        "               specials:'list[str]' = ['<unk>', '<pad>'],\n",
        "               vectors_length:'int' = -1) -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    words: list[str]\n",
        "      The unique words contained in the vocabulary\n",
        "    pretrained_vectors: torchtext.vocab.Vectors\n",
        "      the pretrained embedding containing the Tensors that will be used for the embedding. If there are words included in this class not included in the `words` list they will be added to the vocabulary too.\n",
        "      It can be None but, in this case, a 0 Tensor will be created for the embedding. In the case this value is None, the `vectors_length` is mandatory.\n",
        "    specials: list[str]\n",
        "      A list that contains the special tokens that will be added to the vocabulary. This token will be the first tokens in the resulting list.\n",
        "    vectors_length: int\n",
        "      This parameter is mandatory only if the `pretrained_vectors` parameter is None. It represent the length of each Tensor used in the embedding.\n",
        "    \"\"\"\n",
        "    self.__word2idx = {}\n",
        "    self.idx2word = []\n",
        "    self.vectors = []\n",
        "\n",
        "    self.pre_trained = pretrained_vectors != None\n",
        "\n",
        "    pre_keys = []\n",
        "    pre_vectors = {}\n",
        "\n",
        "    if self.pre_trained:\n",
        "      pre_keys = pretrained_vectors.stoi.keys()\n",
        "      pre_vectors = pretrained_vectors\n",
        "\n",
        "    self.length = vectors_length\n",
        "    if self.pre_trained:\n",
        "      self.length = len(pretrained_vectors.vectors[0])\n",
        "      if vectors_length != -1 and self.length != vectors_length:\n",
        "        raise Exception(f\"vectors_length {vectors_length} incompatible with length of pretrained_vectors {self.length}. Consider removing the vector length property\")\n",
        "    if self.length == -1:\n",
        "      raise Exception(\"either a the pretrained_vectors or the vectors_length properties should be provided\")\n",
        "\n",
        "    idx = 0\n",
        "\n",
        "    for word in specials:\n",
        "      self.__add_word(word, idx, pre_keys, pre_vectors)\n",
        "      idx += 1\n",
        "\n",
        "    for word in pre_keys:\n",
        "      self.__add_word(word, idx, pre_keys, pre_vectors)\n",
        "      idx += 1\n",
        "\n",
        "    for word in words:\n",
        "      if not word in self.__word2idx:\n",
        "        self.__add_word(word, idx, pre_keys, pre_vectors)\n",
        "        idx += 1\n",
        "\n",
        "    self.dim = idx\n",
        "    self.vectors = torch.stack(self.vectors)\n",
        "\n",
        "  def word2idx(self, word):\n",
        "    if word in self.__word2idx:\n",
        "      return self.__word2idx[word]\n",
        "    return self.__word2idx['<unk>']\n",
        "\n",
        "  def __add_word(self, word:'str', idx:'int', pre_keys:'list', pre_vectors: 'dict') -> None:\n",
        "      self.__word2idx[word] = idx\n",
        "      self.idx2word.append(word)\n",
        "      self.vectors.append(pre_vectors[word] if word in pre_keys else zeros(self.length))"
      ],
      "metadata": {
        "id": "5ovAQ8a33uFU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = Vocabulary(np.unique(train_data['word/symbol']), glove)"
      ],
      "metadata": {
        "id": "ujVbEWbDVTk_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary.word2idx('hello'))\n",
        "print(vocabulary.idx2word[13077])\n",
        "print(vocabulary.vectors[13077])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeOkToIgXNdA",
        "outputId": "67115dde-caf1-40f6-961a-7afe7478ad90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13077\n",
            "hello\n",
            "tensor([ 0.2669,  0.3963,  0.6169, -0.7745, -0.1039,  0.2670,  0.2788,  0.3099,\n",
            "         0.0055, -0.0853,  0.7360, -0.0984,  0.5479, -0.0303,  0.3348,  0.1409,\n",
            "        -0.0070,  0.3257,  0.2290,  0.4656, -0.1953,  0.3749, -0.7139, -0.5178,\n",
            "         0.7704,  1.0881, -0.6601, -0.1623,  0.9119,  0.2105,  0.0475,  1.0019,\n",
            "         1.1133,  0.7009, -0.0870,  0.4757,  0.1636, -0.4447,  0.4469, -0.9382,\n",
            "         0.0131,  0.0860, -0.6746,  0.4966, -0.0378, -0.1104, -0.2861,  0.0746,\n",
            "        -0.3153, -0.0938, -0.5707,  0.6686,  0.4531, -0.3415, -0.7166, -0.7527,\n",
            "         0.0752,  0.5790, -0.1191, -0.1138, -0.1003,  0.7134, -1.1574, -0.7403,\n",
            "         0.4045,  0.1802,  0.2145,  0.3764,  0.1124, -0.5364, -0.0251,  0.3189,\n",
            "        -0.2501, -0.6328, -0.0118,  1.3770,  0.8601,  0.2048, -0.3681, -0.6887,\n",
            "         0.5351, -0.4656,  0.2739,  0.4118, -0.8540, -0.0463,  0.1130, -0.2733,\n",
            "         0.1564, -0.2033,  0.5359,  0.5978,  0.6047,  0.1373,  0.4223, -0.6128,\n",
            "        -0.3849,  0.3584, -0.4846,  0.3073])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing and preparation\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i_CHOUNA8oMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PosEncoding\n",
        "It is a similar class to Vocabulary but for the position. It works basically the same but with the one-hot-encoding instead of the embedding."
      ],
      "metadata": {
        "id": "J9_H2yulB7Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PosEncoding:\n",
        "  \"\"\"\n",
        "  A class containing all the pos values used in the training.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  pos2idx : Dict\n",
        "    Maps every pos to the corresponding token.\n",
        "  idx2pos : List[str]\n",
        "    Works as the inverse function to `pos2idx` mapping back every token to the corresponding pos.\n",
        "  encoding : list[torch.Tensor]\n",
        "    maps every pos to the corresponding one-hot-encoded vector.\n",
        "  dim : int\n",
        "    contains the size of the vocabulary.\n",
        "  \"\"\"\n",
        "  def __init__(self, pos_labels: 'list[str]', specials:'list[str]' = ['<pad>']) -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    pos_labels: list[str]\n",
        "      The unique words contained in the vocabulary\n",
        "    specials: list[str]\n",
        "      A list that contains the special pos that will be added to the vocabulary.\n",
        "    \"\"\"\n",
        "    self.pos2idx = {}\n",
        "    self.idx2pos = []\n",
        "    self.encoding = []\n",
        "    starting_tensor = [0 for _ in range(len(pos_labels) + len(specials))]\n",
        "    idx = 0\n",
        "\n",
        "    for pos in specials:\n",
        "      self.__add_pos(pos, idx, starting_tensor)\n",
        "      idx += 1\n",
        "\n",
        "    for pos in pos_labels:\n",
        "      self.__add_pos(pos, idx, starting_tensor)\n",
        "      idx += 1\n",
        "\n",
        "\n",
        "  def __add_pos(self, pos:'str', idx:'int', starting_tensor:'torch.Tensor'):\n",
        "    self.pos2idx[pos] = idx\n",
        "    self.idx2pos.append(pos)\n",
        "    self.encoding.append(starting_tensor.copy())\n",
        "    self.encoding[idx][idx] = 1\n"
      ],
      "metadata": {
        "id": "AXhmgSD19yGf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y encoding"
      ],
      "metadata": {
        "id": "j4KVceH88sfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_labels = np.unique(train_data['pos label'])\n",
        "number_of_pos_labels = len(pos_labels)\n",
        "print(f'there are {number_of_pos_labels} unique pos label values: ' + \"\\n -\" + \"\\n- \".join(pos_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGudSj4L8yS5",
        "outputId": "8c133c8a-617c-42aa-e03c-f6979c24e638"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 45 unique pos label values: \n",
            " -#\n",
            "- $\n",
            "- ''\n",
            "- ,\n",
            "- -LRB-\n",
            "- -RRB-\n",
            "- .\n",
            "- :\n",
            "- CC\n",
            "- CD\n",
            "- DT\n",
            "- EX\n",
            "- FW\n",
            "- IN\n",
            "- JJ\n",
            "- JJR\n",
            "- JJS\n",
            "- LS\n",
            "- MD\n",
            "- NN\n",
            "- NNP\n",
            "- NNPS\n",
            "- NNS\n",
            "- PDT\n",
            "- POS\n",
            "- PRP\n",
            "- PRP$\n",
            "- RB\n",
            "- RBR\n",
            "- RBS\n",
            "- RP\n",
            "- SYM\n",
            "- TO\n",
            "- UH\n",
            "- VB\n",
            "- VBD\n",
            "- VBG\n",
            "- VBN\n",
            "- VBP\n",
            "- VBZ\n",
            "- WDT\n",
            "- WP\n",
            "- WP$\n",
            "- WRB\n",
            "- ``\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoding = PosEncoding(pos_labels)"
      ],
      "metadata": {
        "id": "8eRA4uJaCLyC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_encoding.pos2idx['NNP'])\n",
        "print(pos_encoding.idx2pos[21])\n",
        "print(pos_encoding.encoding[21])\n",
        "print(pos_encoding.encoding[21][21])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu9QGLrlCiYt",
        "outputId": "afc1100e-cf74-4a8b-d61a-cdc364126c1d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "NNP\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(input_data:'pd.Dataframe', vocab:'Vocabulary', encoding:'PosEncoding', sentence_split:'bool'=True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "    A function that returns a tuple containing two Tensor, the first containg the tokenized words of the dataframe, the second containing the encoded labels of the dataframe.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data: pd.Dataframe\n",
        "      The dataframe containing the data to transform. The value of the words will be checked in the 'word/symbol' column while the label will be cheked in the pos label one. If the sentence_split parameter is false, the elements will be grouped by the source file column.\n",
        "    vocab: Vocabulary\n",
        "      The vocabulary used to tokenize the words.\n",
        "    encoding: PosEncoding\n",
        "      The encoding of the labels.\n",
        "    sentence_split: bool\n",
        "      It tells how to split the sentences: if tru, they will be split by the '.' token, if false they will be split by the source file.\n",
        "  \"\"\"\n",
        "  if sentence_split:\n",
        "    return transform_data_by_sentences(input_data, vocab, encoding)\n",
        "  return transform_data_by_files(input_data, vocab, encoding)\n",
        "\n",
        "def tail_pad(x, tot_len, pad_element):\n",
        "  l = len(x)\n",
        "  return x + [pad_element for _ in range(tot_len - l)]\n",
        "\n",
        "def transform_data_by_sentences(input_data:'pd.Dataframe', vocab:'Vocabulary', encoding:'PosEncoding') -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  out_lists_x = [[]]\n",
        "  out_lists_y = [[]]\n",
        "  idx = 0\n",
        "  for i in range(input_data.shape[0]):\n",
        "    row = input_data.iloc[i, :]\n",
        "    out_lists_x[idx].append(row['word/symbol'])\n",
        "    out_lists_y[idx].append(row['pos label'])\n",
        "    if row['word/symbol'] == '.':\n",
        "      out_lists_x.append([])\n",
        "      out_lists_y.append([])\n",
        "      idx += 1\n",
        "  max_len = 0\n",
        "  for sentence in out_lists_x:\n",
        "    if len(sentence) > max_len:\n",
        "      max_len = len(sentence)\n",
        "  for i in range(len(out_lists_x)):\n",
        "    out_lists_x[i] = tail_pad(out_lists_x[i], max_len, '<pad>')\n",
        "    out_lists_y[i] = tail_pad(out_lists_y[i], max_len, '<pad>')\n",
        "    token_words = []\n",
        "    for word in out_lists_x[i]:\n",
        "      token_words.append(vocab.word2idx(word))\n",
        "    encoded_y = []\n",
        "    for pos in out_lists_y[i]:\n",
        "      encoded_y.append(encoding.encoding[encoding.pos2idx[pos]])\n",
        "    out_lists_x[i] = token_words\n",
        "    out_lists_y[i] = encoded_y\n",
        "  return torch.LongTensor(out_lists_x), torch.Tensor(out_lists_y)\n",
        "\n",
        "def transform_data_by_files(input_data:'pd.Dataframe', vocab:'Vocabulary', encoding:'PosEncoding') -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  out_lists_x = {}\n",
        "  out_lists_y = {}\n",
        "  for i in range(input_data.shape[0]):\n",
        "    row = input_data.iloc[i, :]\n",
        "    source = row['source file']\n",
        "    if not source in out_lists_x:\n",
        "      out_lists_x[source] = []\n",
        "      out_lists_y[source] = []\n",
        "    out_lists_x[source].append(row['word/symbol'])\n",
        "    out_lists_y[source].append(row['pos label'])\n",
        "  max_len = 0\n",
        "  for key in out_lists_x.keys():\n",
        "    if len(out_lists_x[key]) > max_len:\n",
        "      max_len = len(out_lists_x[key])\n",
        "  for key in out_lists_x.keys():\n",
        "    out_lists_x[key] = tail_pad(out_lists_x[key], max_len, '<pad>')\n",
        "    out_lists_y[key] = tail_pad(out_lists_y[key], max_len, '<pad>')\n",
        "    token_words = []\n",
        "    for word in out_lists_x[key]:\n",
        "      token_words.append(vocab.word2idx(word))\n",
        "    encoded_y = []\n",
        "    for pos in out_lists_y[key]:\n",
        "      encoded_y.append(encoding.encoding[encoding.pos2idx[pos]])\n",
        "    out_lists_x[key] = token_words\n",
        "    out_lists_y[key] = encoded_y\n",
        "  return torch.LongTensor([ out_lists_x[key] for key in out_lists_x.keys()]), torch.Tensor([out_lists_y[key] for key in out_lists_y.keys()])"
      ],
      "metadata": {
        "id": "5gXfKc2TJ-pd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = transform_data(train_data, vocabulary, pos_encoding)\n",
        "x_validation, y_validation = transform_data(validation_data, vocabulary, pos_encoding)"
      ],
      "metadata": {
        "id": "06j2xmDpQU3n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJHoUF-PWgx"
      },
      "source": [
        "# [Task 3 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your neural POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_bPdqU-PWg0"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
        "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
        "\n",
        "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
        "\n",
        "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device: %s' % device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6gHqF1O9xYd",
        "outputId": "1b331088-dcb9-4e20-b00f-92033e055b15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Layer\n",
        "\n",
        "A very simple layer. It creates a LSTM that can be used with the `NeuralNetwork` class."
      ],
      "metadata": {
        "id": "DyZr-MXqE2pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  A very simple layer. It creates a LSTM that can be used with the `NeuralNetwork` class.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size:'int', hidden_size:'int', bidirectional:'bool') -> None:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size: int\n",
        "      The size of the input to the LSTM layer.\n",
        "    hidden_size: int\n",
        "      The number of LSTM layers.\n",
        "    bidirectional: bool\n",
        "      If the LSTM layer are birectional or not.\n",
        "    \"\"\"\n",
        "    super(LSTMLayer, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, _ = self.lstm(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "XmlrTkavE68H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed Layer\n",
        "\n",
        "This layer handles the embedding of the tokens. It contains two embedding layers: the first assumes that there are pretrained vectors to use \\(vector given as arguments to the `__init__` function\\), the second one covers the values that are not present in the pretrained vector. The second embedding layer will be initialized with random values.\n",
        "\n",
        "The first embedding layer can be frozen in order to avoid training it."
      ],
      "metadata": {
        "id": "wmEFLqd3FITE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How the replace Embedding layer works\n",
        "![img](https://drive.google.com/uc?export=view&id=1tC7yKZwKDkv6RkBK4nU5kz3ckKocnJf4)"
      ],
      "metadata": {
        "id": "WKNXUbVdGwjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedLayer_replace(nn.Module):\n",
        "  def __init__(self, vocabulary, embedding_dim):\n",
        "        super(EmbedLayer_replace, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocabulary.vectors)\n",
        "        self.oov_embedding = nn.Embedding(vocabulary.dim, embedding_dim)\n",
        "        nn.init.uniform_(self.oov_embedding.weight, -1.0, 1.0)\n",
        "\n",
        "  def freeze(self, freeze:'bool'):\n",
        "    self.embedding.freeze = freeze\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    mask = (embedded[:, :, -1] == 0).nonzero()\n",
        "\n",
        "    embedded[mask[:, 0], mask[:, 1], :] = self.oov_embedding(x[mask[:, 0], mask[:, 1]])\n",
        "    return embedded\n"
      ],
      "metadata": {
        "id": "2_2LSqC0GENZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This second version of the embedding works a bit differently: It sums the two embedding values instead of replacing the oov words only. The second random embedding should \\(hopefully\\) work as \"fine-tuning\" layer for training words and be the only embedding for non-pretrained words. We should try and see which version works the best  "
      ],
      "metadata": {
        "id": "duNiM5h_gXoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How the sum Embedding layer works\n",
        "![img](https://drive.google.com/uc?export=view&id=1RrF4tv5YD7gNgjzTJXpeUq3LgQQGAAyH)"
      ],
      "metadata": {
        "id": "tAs47oOXX_Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedLayer_sum(nn.Module):\n",
        "  def __init__(self, vocabulary, embedding_dim):\n",
        "        super(EmbedLayer_sum, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocabulary.vectors)\n",
        "        self.oov_embedding = nn.Embedding(vocabulary.dim, embedding_dim)\n",
        "        nn.init.uniform_(self.oov_embedding.weight, -1.0, 1.0)\n",
        "\n",
        "  def freeze(self, freeze:'bool'):\n",
        "    self.embedding.freeze = freeze\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    pre_trained_embedded = self.embedding(x)\n",
        "\n",
        "    oov_embeds = self.oov_embedding(x)\n",
        "    return pre_trained_embedded + oov_embeds"
      ],
      "metadata": {
        "id": "1fs__rBUgBAR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NeuralNetwork\n",
        "This class is a really simple class that helps with crating a neural network. The constructor needs the optimizer and the loss that are gonna be useful for for training and the device on which the network will be trained. Each parameter can be updated in a second moment.\n",
        "### Methods\n",
        " - The add method can be used to add layers to the nn.\n",
        " - The compile method can be used to actually create the nn \\(the order of the layers will be the same as the order they have been passed to the add method\\)\n",
        " - The train method can be used to train the nn"
      ],
      "metadata": {
        "id": "CApiISQ1D18z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  \"\"\"\n",
        "  This class implements a simple interface to get a working neural network using pytorch.\n",
        "  \"\"\"\n",
        "  def __init__(self, optimizer = torch.optim.Adam, loss = nn.CrossEntropyLoss(), device:'str' = 'cpu'):\n",
        "      \"\"\"\n",
        "      Parameters\n",
        "      ----------\n",
        "      optimizer:\n",
        "        The optimizer to use while training, default to Adam.\n",
        "      loss:\n",
        "        The loss function to use while training, default to crossentropy\n",
        "      device: str\n",
        "        The device where to train end evaluate the neural network. It must be either cpu or a valid pytorch device (e.g. cuda). Default to cpu\n",
        "      \"\"\"\n",
        "      self.layers = []\n",
        "      self.net = None\n",
        "      self.optimizer = optimizer\n",
        "      self.loss = loss\n",
        "      self.device = device\n",
        "\n",
        "  def add(self, *layer:'nn.Module'):\n",
        "    \"\"\"\n",
        "      A function to add layers to the Neural network. The layers must be presentend in the order you want them to be called.\n",
        "      Parameters\n",
        "      ----------\n",
        "      layer: nn.Module\n",
        "        An arbitrary amount of layers to be added to the Neural network. The layers must be presentend in the order you want them to be called.\n",
        "    \"\"\"\n",
        "    if not self.net is None:\n",
        "      print(\"WARNING: the neural network has already been built. If you want the added layer to be added to the built network please rebuild it.\")\n",
        "    self.layers += layer\n",
        "\n",
        "  def compile(self):\n",
        "    \"\"\"\n",
        "      Call this method before using the Neural network for inference or training. It builds the actual neural network.\n",
        "    \"\"\"\n",
        "    if not self.net is None:\n",
        "      print(\"WARNING: the previous network will be discarded. Please retrain the network before using it for inference.\")\n",
        "    self.net = nn.Sequential(*self.layers)\n",
        "    self.net = self.net.to(self.device)\n",
        "    return self\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "     return f\"{self.net}\"\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "  def train(self, train_loader:'torch.utils.data.DataLoader', validation_loader:'torch.utils.data.DataLoader', learning_rate:'float'=.1, epochs:'int'=10) -> Tuple[list[float], list[float], list[float], list[float]]:\n",
        "    \"\"\"\n",
        "      A simple training loop for the neural network. It returns the epochs loss and accuracy history both on the training and the validation set. The tuple will be formatted as:\n",
        "      train loss, train accuracy, val loss, val accuracy\n",
        "      Parameters\n",
        "      ----------\n",
        "      train_loader: torch.utils.data.DataLoader\n",
        "        A dataloader containing the dataset that will be used for training the network\n",
        "      validation_loader: torch.utils.data.DataLoader\n",
        "        A dataloader containing the dataset that will be used for validate the network at the end of each epoch\n",
        "      learning_rate: float\n",
        "        The learning rate that will be used in the optimizer to train the network. Default to .1\n",
        "      epochs:\n",
        "        The number of training epochs, default to 10.\n",
        "    \"\"\"\n",
        "    net = self.net\n",
        "    optimizer = self.optimizer(net.parameters(), learning_rate)\n",
        "\n",
        "    train_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "\n",
        "    start_ts = time.time()\n",
        "\n",
        "    total_batch = int(len(train_loader.dataset) / train_loader.batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = self.loss(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predicted_classes = torch.max(outputs, len(outputs.shape) - 1)[1].view(-1)\n",
        "            normal_labels = torch.max(labels, len(labels.shape) - 1)[1].view(-1)\n",
        "            accuracy = self.__compute_accuracy(predicted_classes.cpu(), normal_labels.cpu())\n",
        "            write(f\"\\rbatch {batch_idx + 1}/{total_batch} ----- loss: {loss.cpu()} ----- accuracy: {accuracy} \")\n",
        "            stdout.flush()\n",
        "\n",
        "        val_accuracy, val_loss = self.__validate(validation_loader)\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "        val_loss_history.append(val_loss)\n",
        "        train_accuracy, train_loss = self.__validate(train_loader)\n",
        "        train_accuracy_history.append(train_accuracy)\n",
        "        train_loss_history.append(train_loss)\n",
        "        out_str = f\"======================================================================================================================================\\nEPOCH {epoch + 1} training loss: {train_loss_history[-1]} - validation loss: {val_loss_history[-1]}\\nEPOCH {epoch + 1} training accuracy: {train_accuracy_history[-1]} - validation accuracy: {val_accuracy_history[-1]}\\n======================================================================================================================================\\n\"\n",
        "        stdout.write(\"\\r\" + \" \" * len(out_str) + \"\\r\")\n",
        "        stdout.flush()\n",
        "        stdout.write(out_str)\n",
        "        stdout.flush()\n",
        "        print()\n",
        "\n",
        "    return train_loss_history, train_accuracy_history, val_loss_history, val_accuracy_history\n",
        "\n",
        "\n",
        "  def __compute_accuracy(self, best_guesses, targets):\n",
        "    num_correct = torch.eq(targets, best_guesses).sum().item()\n",
        "    total_guesses = len(targets)\n",
        "    correct_percentage = num_correct/total_guesses\n",
        "    return correct_percentage\n",
        "\n",
        "  def __validate(self, val_loader):\n",
        "    val_losses = []\n",
        "    val_accuracy = []\n",
        "    net = self.net\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(val_loader):\n",
        "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            loss = self.loss(outputs, labels)\n",
        "            val_losses.append(loss)\n",
        "\n",
        "            predicted_classes = torch.max(outputs, len(outputs.shape) - 1)[1].view(-1)\n",
        "            val_accuracy.append(self.__compute_accuracy(predicted_classes.cpu(), torch.max(labels, len(labels.shape) - 1)[1].view(-1).cpu()))\n",
        "\n",
        "    average_val_loss = sum(val_losses)/(batch_idx+1)\n",
        "    average_val_accuracy = sum(val_accuracy)/len(val_loader)\n",
        "    return average_val_accuracy, average_val_loss"
      ],
      "metadata": {
        "id": "0gRFP9OJ6mWn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_LSTM_SIZE = 500\n",
        "LINEAR_SIZE = 200"
      ],
      "metadata": {
        "id": "qyCEDeUSYuxL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = EmbedLayer_replace(vocabulary, 100)\n",
        "embed_layer.freeze(True)\n",
        "baseline = NeuralNetwork(device=device)\n",
        "baseline.add(embed_layer,\n",
        "          LSTMLayer(100, HIDDEN_LSTM_SIZE, True),\n",
        "          nn.Linear(HIDDEN_LSTM_SIZE * 2, y_train.shape[2]),\n",
        "          nn.Softmax(dim=0)\n",
        "          )\n",
        "print(baseline.compile())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxduiNiFD14l",
        "outputId": "543acd8c-7587-4a5f-9f2d-20a09de4795e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): EmbedLayer_replace(\n",
            "    (embedding): Embedding(402348, 100)\n",
            "    (oov_embedding): Embedding(402348, 100)\n",
            "  )\n",
            "  (1): LSTMLayer(\n",
            "    (lstm): LSTM(100, 500, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (2): Linear(in_features=1000, out_features=46, bias=True)\n",
            "  (3): Softmax(dim=0)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = EmbedLayer_replace(vocabulary, 100)\n",
        "embed_layer.freeze(True)\n",
        "Model1 = NeuralNetwork()\n",
        "Model1.add(embed_layer,\n",
        "          LSTMLayer(100, HIDDEN_LSTM_SIZE, True),\n",
        "          LSTMLayer(HIDDEN_LSTM_SIZE * 2, HIDDEN_LSTM_SIZE, True),\n",
        "          nn.Linear(HIDDEN_LSTM_SIZE * 2, y_train.shape[2]),\n",
        "          nn.Softmax(dim=0))\n",
        "print(Model1.compile())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uUh_OdCLmmI",
        "outputId": "cbdd9319-0b44-418f-95d2-e856377cae9c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): EmbedLayer_replace(\n",
            "    (embedding): Embedding(402348, 100)\n",
            "    (oov_embedding): Embedding(402348, 100)\n",
            "  )\n",
            "  (1): LSTMLayer(\n",
            "    (lstm): LSTM(100, 500, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (2): LSTMLayer(\n",
            "    (lstm): LSTM(1000, 500, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (3): Linear(in_features=1000, out_features=46, bias=True)\n",
            "  (4): Softmax(dim=0)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = EmbedLayer_replace(vocabulary, 100)\n",
        "embed_layer.freeze(True)\n",
        "model2 = NeuralNetwork()\n",
        "model2.add(embed_layer,\n",
        "          LSTMLayer(100, HIDDEN_LSTM_SIZE, True),\n",
        "          nn.Linear(HIDDEN_LSTM_SIZE * 2, LINEAR_SIZE),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(LINEAR_SIZE, y_train.shape[2]),\n",
        "          nn.Softmax(dim=0))\n",
        "print(model2.compile())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCk7QX0vL77t",
        "outputId": "54088452-c32a-4481-cffe-4ea7f748c4fb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): EmbedLayer_replace(\n",
            "    (embedding): Embedding(402348, 100)\n",
            "    (oov_embedding): Embedding(402348, 100)\n",
            "  )\n",
            "  (1): LSTMLayer(\n",
            "    (lstm): LSTM(100, 150, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (2): Linear(in_features=300, out_features=200, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=200, out_features=46, bias=True)\n",
            "  (5): Softmax(dim=0)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 200\n",
        "train_dataset = Dataset(x_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "validation_dataset = Dataset(x_validation, y_validation)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "okAVljr7ILOM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.train(train_loader, validation_loader, epochs=20)"
      ],
      "metadata": {
        "id": "Sz3veASQEX1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650583d6-1c42-4b94-dcb4-46fab9029b44"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================================================================================\n",
            "EPOCH 1 training loss: 29.99021339416504 - validation loss: 8.548507690429688\n",
            "EPOCH 1 training accuracy: 0.13312832116788323 - validation accuracy: 0.12623319662793347\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 2 training loss: 29.949438095092773 - validation loss: 8.506352424621582\n",
            "EPOCH 2 training accuracy: 0.08344331386861314 - validation accuracy: 0.16201654704944177\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 3 training loss: 29.92738151550293 - validation loss: 8.483741760253906\n",
            "EPOCH 3 training accuracy: 0.0876934306569343 - validation accuracy: 0.14807857712462977\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 4 training loss: 29.916624069213867 - validation loss: 8.47358512878418\n",
            "EPOCH 4 training accuracy: 0.11084491970802919 - validation accuracy: 0.20408549783549781\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 5 training loss: 29.9133358001709 - validation loss: 8.47010326385498\n",
            "EPOCH 5 training accuracy: 0.09239029197080291 - validation accuracy: 0.22850777511961723\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 6 training loss: 29.911182403564453 - validation loss: 8.46706485748291\n",
            "EPOCH 6 training accuracy: 0.11281391240875913 - validation accuracy: 0.21159916837548418\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 7 training loss: 29.9102725982666 - validation loss: 8.465983390808105\n",
            "EPOCH 7 training accuracy: 0.13259859854014597 - validation accuracy: 0.23027155958077014\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 8 training loss: 29.90813636779785 - validation loss: 8.464479446411133\n",
            "EPOCH 8 training accuracy: 0.1765016496350365 - validation accuracy: 0.232751338573707\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 9 training loss: 29.907413482666016 - validation loss: 8.463223457336426\n",
            "EPOCH 9 training accuracy: 0.14586753284671533 - validation accuracy: 0.23520320688083846\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 10 training loss: 29.906330108642578 - validation loss: 8.462557792663574\n",
            "EPOCH 10 training accuracy: 0.12450483211678831 - validation accuracy: 0.22073863636363636\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 11 training loss: 29.906888961791992 - validation loss: 8.463557243347168\n",
            "EPOCH 11 training accuracy: 0.1267384817518248 - validation accuracy: 0.22104337548416494\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 12 training loss: 29.905485153198242 - validation loss: 8.4613676071167\n",
            "EPOCH 12 training accuracy: 0.17234623357664233 - validation accuracy: 0.23553514467988151\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 13 training loss: 29.907062530517578 - validation loss: 8.46457290649414\n",
            "EPOCH 13 training accuracy: 0.1980723211678832 - validation accuracy: 0.23107926065162904\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 14 training loss: 29.907255172729492 - validation loss: 8.464366912841797\n",
            "EPOCH 14 training accuracy: 0.13289369343065693 - validation accuracy: 0.22581510594668489\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 15 training loss: 29.9068603515625 - validation loss: 8.464025497436523\n",
            "EPOCH 15 training accuracy: 0.14803443795620438 - validation accuracy: 0.22165000569605833\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 16 training loss: 29.904937744140625 - validation loss: 8.462148666381836\n",
            "EPOCH 16 training accuracy: 0.14454525547445257 - validation accuracy: 0.22878446115288217\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 17 training loss: 29.904617309570312 - validation loss: 8.461836814880371\n",
            "EPOCH 17 training accuracy: 0.1624789927007299 - validation accuracy: 0.27786355092276144\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 18 training loss: 29.90511131286621 - validation loss: 8.461007118225098\n",
            "EPOCH 18 training accuracy: 0.16917245255474453 - validation accuracy: 0.26068366940077464\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 19 training loss: 29.904666900634766 - validation loss: 8.461531639099121\n",
            "EPOCH 19 training accuracy: 0.17994960583941605 - validation accuracy: 0.2634120813397129\n",
            "======================================================================================================================================\n",
            "\n",
            "======================================================================================================================================\n",
            "EPOCH 20 training loss: 29.905332565307617 - validation loss: 8.46175765991211\n",
            "EPOCH 20 training accuracy: 0.19174851094890513 - validation accuracy: 0.29156342560947823\n",
            "======================================================================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor(29.9902, device='cuda:0'),\n",
              "  tensor(29.9494, device='cuda:0'),\n",
              "  tensor(29.9274, device='cuda:0'),\n",
              "  tensor(29.9166, device='cuda:0'),\n",
              "  tensor(29.9133, device='cuda:0'),\n",
              "  tensor(29.9112, device='cuda:0'),\n",
              "  tensor(29.9103, device='cuda:0'),\n",
              "  tensor(29.9081, device='cuda:0'),\n",
              "  tensor(29.9074, device='cuda:0'),\n",
              "  tensor(29.9063, device='cuda:0'),\n",
              "  tensor(29.9069, device='cuda:0'),\n",
              "  tensor(29.9055, device='cuda:0'),\n",
              "  tensor(29.9071, device='cuda:0'),\n",
              "  tensor(29.9073, device='cuda:0'),\n",
              "  tensor(29.9069, device='cuda:0'),\n",
              "  tensor(29.9049, device='cuda:0'),\n",
              "  tensor(29.9046, device='cuda:0'),\n",
              "  tensor(29.9051, device='cuda:0'),\n",
              "  tensor(29.9047, device='cuda:0'),\n",
              "  tensor(29.9053, device='cuda:0')],\n",
              " [0.13312832116788323,\n",
              "  0.08344331386861314,\n",
              "  0.0876934306569343,\n",
              "  0.11084491970802919,\n",
              "  0.09239029197080291,\n",
              "  0.11281391240875913,\n",
              "  0.13259859854014597,\n",
              "  0.1765016496350365,\n",
              "  0.14586753284671533,\n",
              "  0.12450483211678831,\n",
              "  0.1267384817518248,\n",
              "  0.17234623357664233,\n",
              "  0.1980723211678832,\n",
              "  0.13289369343065693,\n",
              "  0.14803443795620438,\n",
              "  0.14454525547445257,\n",
              "  0.1624789927007299,\n",
              "  0.16917245255474453,\n",
              "  0.17994960583941605,\n",
              "  0.19174851094890513],\n",
              " [tensor(8.5485, device='cuda:0'),\n",
              "  tensor(8.5064, device='cuda:0'),\n",
              "  tensor(8.4837, device='cuda:0'),\n",
              "  tensor(8.4736, device='cuda:0'),\n",
              "  tensor(8.4701, device='cuda:0'),\n",
              "  tensor(8.4671, device='cuda:0'),\n",
              "  tensor(8.4660, device='cuda:0'),\n",
              "  tensor(8.4645, device='cuda:0'),\n",
              "  tensor(8.4632, device='cuda:0'),\n",
              "  tensor(8.4626, device='cuda:0'),\n",
              "  tensor(8.4636, device='cuda:0'),\n",
              "  tensor(8.4614, device='cuda:0'),\n",
              "  tensor(8.4646, device='cuda:0'),\n",
              "  tensor(8.4644, device='cuda:0'),\n",
              "  tensor(8.4640, device='cuda:0'),\n",
              "  tensor(8.4621, device='cuda:0'),\n",
              "  tensor(8.4618, device='cuda:0'),\n",
              "  tensor(8.4610, device='cuda:0'),\n",
              "  tensor(8.4615, device='cuda:0'),\n",
              "  tensor(8.4618, device='cuda:0')],\n",
              " [0.12623319662793347,\n",
              "  0.16201654704944177,\n",
              "  0.14807857712462977,\n",
              "  0.20408549783549781,\n",
              "  0.22850777511961723,\n",
              "  0.21159916837548418,\n",
              "  0.23027155958077014,\n",
              "  0.232751338573707,\n",
              "  0.23520320688083846,\n",
              "  0.22073863636363636,\n",
              "  0.22104337548416494,\n",
              "  0.23553514467988151,\n",
              "  0.23107926065162904,\n",
              "  0.22581510594668489,\n",
              "  0.22165000569605833,\n",
              "  0.22878446115288217,\n",
              "  0.27786355092276144,\n",
              "  0.26068366940077464,\n",
              "  0.2634120813397129,\n",
              "  0.29156342560947823])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYMHZEeMPWg1"
      },
      "source": [
        "# [Task 4 - 1.0 points] Metrics\n",
        "\n",
        "Before training the models, you are tasked to define the evaluation metrics for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0AFgk7qPWg2"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
        "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n",
        "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRbAjydQPWg2"
      },
      "source": [
        "**Note**: What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rt8yXVMPWg2"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1SoEvCkPWg3"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_DGswu1PWg3"
      },
      "source": [
        "# [Task 6 - 1.0 points] Error Analysis\n",
        "\n",
        "You are tasked to evaluate your best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo7c7KDbPWg4"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Compare the errors made on the validation and test sets.\n",
        "* Aggregate model errors into categories (if possible)\n",
        "* Comment the about errors and propose possible solutions on how to address them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6O5TuxPWg4"
      },
      "source": [
        "# [Task 7 - 1.0 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUJ9UKMZPWg5"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkUvb_LkPWg5"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62xEDwFPWg6"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "017zL6z1PWg6"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYjOdgbMPWg6"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUsAIjyYPWg7"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "\n",
        "However, you are **free** to play with their hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62oRvXM2PWg7"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULplO2tTPWg8"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNEBeS9HPWg8"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Model performance on most/less frequent classes.\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snebFw_VPWg9"
      },
      "source": [
        "### Punctuation\n",
        "\n",
        "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
        "\n",
        "You should **ignore** it during metrics computation.\n",
        "\n",
        "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9sTFDJrPWg9"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
