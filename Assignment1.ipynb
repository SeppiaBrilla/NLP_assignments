{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: POS tagging, Sequence labelling, RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK4tXYIZPWgv"
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5DpJrkwPWgv"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the task of POS tagging.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/pos_tagging.png\" alt=\"POS tagging\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YqCsaSYMRAN1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import zeros\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import numpy as np\n",
    "from sys import stdout\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "flush, write = stdout.flush , stdout.write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6UV8pgMPWgw"
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus\n",
    "\n",
    "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
    "\n",
    "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
    "\n",
    "### Example\n",
    "\n",
    "```Pierre\tNNP\t2\n",
    "Vinken\tNNP\t8\n",
    ",\t,\t2\n",
    "61\tCD\t5\n",
    "years\tNNS\t6\n",
    "old\tJJ\t2\n",
    ",\t,\t2\n",
    "will\tMD\t0\n",
    "join\tVB\t8\n",
    "the\tDT\t11\n",
    "board\tNN\t9\n",
    "as\tIN\t9\n",
    "a\tDT\t15\n",
    "nonexecutive\tJJ\t15\n",
    "director\tNN\t12\n",
    "Nov.\tNNP\t9\n",
    "29\tCD\t16\n",
    ".\t.\t8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Evf0B3k4Q20o"
   },
   "source": [
    "### Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAoEoejnQVc8",
    "outputId": "fda7b774-84ae-4230-ba67-27df9b65046e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  446k  100  446k    0     0   572k      0 --:--:-- --:--:-- --:--:--  571k\n",
      "Archive:  dependency_treebank.zip\n",
      "  inflating: dependency_treebank/wsj_0093.dp  \n",
      "  inflating: dependency_treebank/wsj_0065.dp  \n",
      "  inflating: dependency_treebank/wsj_0039.dp  \n",
      "  inflating: dependency_treebank/wsj_0182.dp  \n",
      "  inflating: dependency_treebank/wsj_0186.dp  \n",
      "  inflating: dependency_treebank/wsj_0041.dp  \n",
      "  inflating: dependency_treebank/wsj_0018.dp  \n",
      "  inflating: dependency_treebank/wsj_0105.dp  \n",
      "  inflating: dependency_treebank/wsj_0149.dp  \n",
      "  inflating: dependency_treebank/wsj_0194.dp  \n",
      "  inflating: dependency_treebank/wsj_0055.dp  \n",
      "  inflating: dependency_treebank/wsj_0187.dp  \n",
      "  inflating: dependency_treebank/wsj_0143.dp  \n",
      "  inflating: dependency_treebank/wsj_0052.dp  \n",
      "  inflating: dependency_treebank/wsj_0064.dp  \n",
      "  inflating: dependency_treebank/wsj_0179.dp  \n",
      "  inflating: dependency_treebank/wsj_0195.dp  \n",
      "  inflating: dependency_treebank/wsj_0051.dp  \n",
      "  inflating: dependency_treebank/wsj_0059.dp  \n",
      "  inflating: dependency_treebank/wsj_0109.dp  \n",
      "  inflating: dependency_treebank/wsj_0074.dp  \n",
      "  inflating: dependency_treebank/wsj_0089.dp  \n",
      "  inflating: dependency_treebank/wsj_0108.dp  \n",
      "  inflating: dependency_treebank/wsj_0104.dp  \n",
      "  inflating: dependency_treebank/wsj_0164.dp  \n",
      "  inflating: dependency_treebank/wsj_0024.dp  \n",
      "  inflating: dependency_treebank/wsj_0008.dp  \n",
      "  inflating: dependency_treebank/wsj_0101.dp  \n",
      "  inflating: dependency_treebank/wsj_0132.dp  \n",
      "  inflating: dependency_treebank/wsj_0028.dp  \n",
      "  inflating: dependency_treebank/wsj_0184.dp  \n",
      "  inflating: dependency_treebank/wsj_0082.dp  \n",
      "  inflating: dependency_treebank/wsj_0114.dp  \n",
      "  inflating: dependency_treebank/wsj_0061.dp  \n",
      "  inflating: dependency_treebank/wsj_0190.dp  \n",
      "  inflating: dependency_treebank/wsj_0034.dp  \n",
      "  inflating: dependency_treebank/wsj_0043.dp  \n",
      "  inflating: dependency_treebank/wsj_0044.dp  \n",
      "  inflating: dependency_treebank/wsj_0021.dp  \n",
      "  inflating: dependency_treebank/wsj_0005.dp  \n",
      "  inflating: dependency_treebank/wsj_0112.dp  \n",
      "  inflating: dependency_treebank/wsj_0167.dp  \n",
      "  inflating: dependency_treebank/wsj_0042.dp  \n",
      "  inflating: dependency_treebank/wsj_0168.dp  \n",
      "  inflating: dependency_treebank/wsj_0185.dp  \n",
      "  inflating: dependency_treebank/wsj_0057.dp  \n",
      "  inflating: dependency_treebank/wsj_0015.dp  \n",
      "  inflating: dependency_treebank/wsj_0116.dp  \n",
      "  inflating: dependency_treebank/wsj_0135.dp  \n",
      "  inflating: dependency_treebank/wsj_0175.dp  \n",
      "  inflating: dependency_treebank/wsj_0171.dp  \n",
      "  inflating: dependency_treebank/wsj_0068.dp  \n",
      "  inflating: dependency_treebank/wsj_0080.dp  \n",
      "  inflating: dependency_treebank/wsj_0035.dp  \n",
      "  inflating: dependency_treebank/wsj_0181.dp  \n",
      "  inflating: dependency_treebank/wsj_0177.dp  \n",
      "  inflating: dependency_treebank/wsj_0102.dp  \n",
      "  inflating: dependency_treebank/wsj_0137.dp  \n",
      "  inflating: dependency_treebank/wsj_0022.dp  \n",
      "  inflating: dependency_treebank/wsj_0176.dp  \n",
      "  inflating: dependency_treebank/wsj_0180.dp  \n",
      "  inflating: dependency_treebank/wsj_0121.dp  \n",
      "  inflating: dependency_treebank/wsj_0128.dp  \n",
      "  inflating: dependency_treebank/wsj_0036.dp  \n",
      "  inflating: dependency_treebank/wsj_0071.dp  \n",
      "  inflating: dependency_treebank/wsj_0091.dp  \n",
      "  inflating: dependency_treebank/wsj_0076.dp  \n",
      "  inflating: dependency_treebank/wsj_0123.dp  \n",
      "  inflating: dependency_treebank/wsj_0075.dp  \n",
      "  inflating: dependency_treebank/wsj_0131.dp  \n",
      "  inflating: dependency_treebank/wsj_0050.dp  \n",
      "  inflating: dependency_treebank/wsj_0136.dp  \n",
      "  inflating: dependency_treebank/wsj_0161.dp  \n",
      "  inflating: dependency_treebank/wsj_0033.dp  \n",
      "  inflating: dependency_treebank/wsj_0188.dp  \n",
      "  inflating: dependency_treebank/wsj_0085.dp  \n",
      "  inflating: dependency_treebank/wsj_0014.dp  \n",
      "  inflating: dependency_treebank/wsj_0073.dp  \n",
      "  inflating: dependency_treebank/wsj_0199.dp  \n",
      "  inflating: dependency_treebank/wsj_0120.dp  \n",
      "  inflating: dependency_treebank/wsj_0178.dp  \n",
      "  inflating: dependency_treebank/wsj_0122.dp  \n",
      "  inflating: dependency_treebank/wsj_0040.dp  \n",
      "  inflating: dependency_treebank/wsj_0020.dp  \n",
      "  inflating: dependency_treebank/wsj_0153.dp  \n",
      "  inflating: dependency_treebank/wsj_0107.dp  \n",
      "  inflating: dependency_treebank/wsj_0017.dp  \n",
      "  inflating: dependency_treebank/wsj_0140.dp  \n",
      "  inflating: dependency_treebank/wsj_0038.dp  \n",
      "  inflating: dependency_treebank/wsj_0031.dp  \n",
      "  inflating: dependency_treebank/wsj_0165.dp  \n",
      "  inflating: dependency_treebank/wsj_0146.dp  \n",
      "  inflating: dependency_treebank/wsj_0090.dp  \n",
      "  inflating: dependency_treebank/wsj_0001.dp  \n",
      "  inflating: dependency_treebank/wsj_0148.dp  \n",
      "  inflating: dependency_treebank/wsj_0097.dp  \n",
      "  inflating: dependency_treebank/wsj_0009.dp  \n",
      "  inflating: dependency_treebank/wsj_0173.dp  \n",
      "  inflating: dependency_treebank/wsj_0111.dp  \n",
      "  inflating: dependency_treebank/wsj_0129.dp  \n",
      "  inflating: dependency_treebank/wsj_0130.dp  \n",
      "  inflating: dependency_treebank/wsj_0047.dp  \n",
      "  inflating: dependency_treebank/wsj_0110.dp  \n",
      "  inflating: dependency_treebank/wsj_0113.dp  \n",
      "  inflating: dependency_treebank/wsj_0147.dp  \n",
      "  inflating: dependency_treebank/wsj_0160.dp  \n",
      "  inflating: dependency_treebank/wsj_0099.dp  \n",
      "  inflating: dependency_treebank/wsj_0003.dp  \n",
      "  inflating: dependency_treebank/wsj_0011.dp  \n",
      "  inflating: dependency_treebank/wsj_0056.dp  \n",
      "  inflating: dependency_treebank/wsj_0069.dp  \n",
      "  inflating: dependency_treebank/wsj_0026.dp  \n",
      "  inflating: dependency_treebank/wsj_0138.dp  \n",
      "  inflating: dependency_treebank/wsj_0029.dp  \n",
      "  inflating: dependency_treebank/wsj_0115.dp  \n",
      "  inflating: dependency_treebank/wsj_0037.dp  \n",
      "  inflating: dependency_treebank/wsj_0019.dp  \n",
      "  inflating: dependency_treebank/wsj_0002.dp  \n",
      "  inflating: dependency_treebank/wsj_0007.dp  \n",
      "  inflating: dependency_treebank/wsj_0158.dp  \n",
      "  inflating: dependency_treebank/wsj_0087.dp  \n",
      "  inflating: dependency_treebank/wsj_0157.dp  \n",
      "  inflating: dependency_treebank/wsj_0083.dp  \n",
      "  inflating: dependency_treebank/wsj_0103.dp  \n",
      "  inflating: dependency_treebank/wsj_0058.dp  \n",
      "  inflating: dependency_treebank/wsj_0054.dp  \n",
      "  inflating: dependency_treebank/wsj_0016.dp  \n",
      "  inflating: dependency_treebank/wsj_0126.dp  \n",
      "  inflating: dependency_treebank/wsj_0198.dp  \n",
      "  inflating: dependency_treebank/wsj_0144.dp  \n",
      "  inflating: dependency_treebank/wsj_0096.dp  \n",
      "  inflating: dependency_treebank/wsj_0086.dp  \n",
      "  inflating: dependency_treebank/wsj_0197.dp  \n",
      "  inflating: dependency_treebank/wsj_0025.dp  \n",
      "  inflating: dependency_treebank/wsj_0100.dp  \n",
      "  inflating: dependency_treebank/wsj_0084.dp  \n",
      "  inflating: dependency_treebank/wsj_0098.dp  \n",
      "  inflating: dependency_treebank/wsj_0106.dp  \n",
      "  inflating: dependency_treebank/wsj_0119.dp  \n",
      "  inflating: dependency_treebank/wsj_0092.dp  \n",
      "  inflating: dependency_treebank/wsj_0134.dp  \n",
      "  inflating: dependency_treebank/wsj_0077.dp  \n",
      "  inflating: dependency_treebank/wsj_0060.dp  \n",
      "  inflating: dependency_treebank/wsj_0172.dp  \n",
      "  inflating: dependency_treebank/wsj_0048.dp  \n",
      "  inflating: dependency_treebank/wsj_0030.dp  \n",
      "  inflating: dependency_treebank/wsj_0192.dp  \n",
      "  inflating: dependency_treebank/wsj_0066.dp  \n",
      "  inflating: dependency_treebank/wsj_0045.dp  \n",
      "  inflating: dependency_treebank/wsj_0155.dp  \n",
      "  inflating: dependency_treebank/wsj_0118.dp  \n",
      "  inflating: dependency_treebank/wsj_0152.dp  \n",
      "  inflating: dependency_treebank/wsj_0012.dp  \n",
      "  inflating: dependency_treebank/wsj_0006.dp  \n",
      "  inflating: dependency_treebank/wsj_0159.dp  \n",
      "  inflating: dependency_treebank/wsj_0163.dp  \n",
      "  inflating: dependency_treebank/wsj_0170.dp  \n",
      "  inflating: dependency_treebank/wsj_0141.dp  \n",
      "  inflating: dependency_treebank/wsj_0117.dp  \n",
      "  inflating: dependency_treebank/wsj_0125.dp  \n",
      "  inflating: dependency_treebank/wsj_0094.dp  \n",
      "  inflating: dependency_treebank/wsj_0169.dp  \n",
      "  inflating: dependency_treebank/wsj_0027.dp  \n",
      "  inflating: dependency_treebank/wsj_0010.dp  \n",
      "  inflating: dependency_treebank/wsj_0162.dp  \n",
      "  inflating: dependency_treebank/wsj_0127.dp  \n",
      "  inflating: dependency_treebank/wsj_0142.dp  \n",
      "  inflating: dependency_treebank/wsj_0046.dp  \n",
      "  inflating: dependency_treebank/wsj_0088.dp  \n",
      "  inflating: dependency_treebank/wsj_0079.dp  \n",
      "  inflating: dependency_treebank/wsj_0174.dp  \n",
      "  inflating: dependency_treebank/wsj_0063.dp  \n",
      "  inflating: dependency_treebank/wsj_0023.dp  \n",
      "  inflating: dependency_treebank/wsj_0004.dp  \n",
      "  inflating: dependency_treebank/wsj_0156.dp  \n",
      "  inflating: dependency_treebank/wsj_0133.dp  \n",
      "  inflating: dependency_treebank/wsj_0032.dp  \n",
      "  inflating: dependency_treebank/wsj_0070.dp  \n",
      "  inflating: dependency_treebank/wsj_0154.dp  \n",
      "  inflating: dependency_treebank/wsj_0095.dp  \n",
      "  inflating: dependency_treebank/wsj_0072.dp  \n",
      "  inflating: dependency_treebank/wsj_0183.dp  \n",
      "  inflating: dependency_treebank/wsj_0081.dp  \n",
      "  inflating: dependency_treebank/wsj_0196.dp  \n",
      "  inflating: dependency_treebank/wsj_0062.dp  \n",
      "  inflating: dependency_treebank/wsj_0124.dp  \n",
      "  inflating: dependency_treebank/wsj_0191.dp  \n",
      "  inflating: dependency_treebank/wsj_0013.dp  \n",
      "  inflating: dependency_treebank/wsj_0078.dp  \n",
      "  inflating: dependency_treebank/wsj_0150.dp  \n",
      "  inflating: dependency_treebank/wsj_0049.dp  \n",
      "  inflating: dependency_treebank/wsj_0189.dp  \n",
      "  inflating: dependency_treebank/wsj_0151.dp  \n",
      "  inflating: dependency_treebank/wsj_0193.dp  \n",
      "  inflating: dependency_treebank/wsj_0067.dp  \n",
      "  inflating: dependency_treebank/wsj_0145.dp  \n",
      "  inflating: dependency_treebank/wsj_0139.dp  \n",
      "  inflating: dependency_treebank/wsj_0166.dp  \n",
      "  inflating: dependency_treebank/wsj_0053.dp  \n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
    "!unzip -o dependency_treebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkhLvBtRPWgw"
   },
   "source": [
    "### Splits\n",
    "\n",
    "The corpus contains 200 documents.\n",
    "\n",
    "   * **Train**: Documents 1-100\n",
    "   * **Validation**: Documents 101-150\n",
    "   * **Test**: Documents 151-199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtoJ6bs_RBnk"
   },
   "source": [
    "### Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "etSG9VDOTq7G"
   },
   "outputs": [],
   "source": [
    "def pad(x, tot_len, pad_val):\n",
    "  l = len(x)\n",
    "  return ''.join([pad_val for _ in range(tot_len - l)]) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LCRhF-RDVrOw"
   },
   "outputs": [],
   "source": [
    "def import_corpus(folder:'str', file_base:'str', corpus_range:'range', separator:'str', data:'object'):\n",
    "  keys = list(data.keys())\n",
    "  data_len = len(keys)\n",
    "  data['source file'] = []\n",
    "  for i in corpus_range:\n",
    "    f = open(f'{folder}/{file_base}{pad(str(i), 4, \"0\")}.dp')\n",
    "    for line in f.readlines():\n",
    "      entry = line.replace('\\n','').split(separator)\n",
    "      if len(entry) >= data_len:\n",
    "        for j in range(data_len):\n",
    "            data[keys[j]].append(entry[j])\n",
    "        data['source file'].append(f'{file_base}{pad(str(i), 4, \"0\")}')\n",
    "    f.close()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8nnJIIQITbFq",
    "outputId": "cefe4faa-8e8b-44f6-96c4-51dd7baccee6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word/symbol</th>\n",
       "      <th>pos label</th>\n",
       "      <th>source file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "      <td>wsj_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "      <td>wsj_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>wsj_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "      <td>wsj_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "      <td>wsj_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47351</th>\n",
       "      <td>challenge</td>\n",
       "      <td>NN</td>\n",
       "      <td>wsj_0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47352</th>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>wsj_0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47353</th>\n",
       "      <td>has</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>wsj_0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47354</th>\n",
       "      <td>faced</td>\n",
       "      <td>VBN</td>\n",
       "      <td>wsj_0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47355</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>wsj_0100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47356 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word/symbol pos label source file\n",
       "0          Pierre       NNP    wsj_0001\n",
       "1          Vinken       NNP    wsj_0001\n",
       "2               ,         ,    wsj_0001\n",
       "3              61        CD    wsj_0001\n",
       "4           years       NNS    wsj_0001\n",
       "...           ...       ...         ...\n",
       "47351   challenge        NN    wsj_0100\n",
       "47352          he       PRP    wsj_0100\n",
       "47353         has       VBZ    wsj_0100\n",
       "47354       faced       VBN    wsj_0100\n",
       "47355           .         .    wsj_0100\n",
       "\n",
       "[47356 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEPARATOR = '\\t'\n",
    "folder = 'dependency_treebank'\n",
    "file_base = 'wsj_'\n",
    "\n",
    "train_data = pd.DataFrame(import_corpus(folder, file_base, range(1,101), SEPARATOR, {'word/symbol':[], 'pos label':[]}))\n",
    "validation_data = pd.DataFrame(import_corpus(folder, file_base, range(101,151), SEPARATOR, {'word/symbol':[], 'pos label':[]}))\n",
    "test_data = pd.DataFrame(import_corpus(folder, file_base, range(151,200), SEPARATOR, {'word/symbol':[], 'pos label':[]}))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WveiIg-UPWgx"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Download** the corpus.\n",
    "* **Encode** the corpus into a pandas.DataFrame object.\n",
    "* **Split** it in training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mhiw3MBkPWgx"
   },
   "source": [
    "# [Task 2 - 0.5 points] Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuqriwPkPWgx"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zv4zgrWuTvUY",
    "outputId": "e8e03aef-c2b2-457b-e085-e1698ee94171"
   },
   "outputs": [],
   "source": [
    "glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KFWwF1YUKkG",
    "outputId": "797ccd80-cec5-48e3-c731-6ec094abbb17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word at position 50: director\n",
      "embedding: \n",
      " tensor([ 2.6554e-01, -7.8286e-01, -8.8447e-02, -1.0131e+00,  9.9533e-01,\n",
      "        -9.4081e-01, -3.6870e-01,  3.3595e-01, -6.6131e-01,  1.8660e-02,\n",
      "        -1.0583e-01, -5.0757e-01,  4.3957e-01,  1.1668e-01,  6.8358e-03,\n",
      "        -1.6246e-01,  8.3221e-01, -2.9842e-02, -5.8107e-01,  5.9797e-01,\n",
      "        -3.4653e-01,  3.3801e-01,  8.3349e-02, -3.3689e-01, -1.5555e-01,\n",
      "         3.0370e-01,  3.6218e-01, -4.9779e-01, -1.0420e-01,  2.3055e-01,\n",
      "        -9.2252e-01,  6.0625e-01, -3.4707e-01,  3.9155e-01, -1.1208e+00,\n",
      "        -5.4766e-02,  9.1888e-02,  1.3057e+00,  1.7112e-01, -4.7524e-01,\n",
      "        -3.8920e-01, -6.1009e-02, -6.0362e-01,  4.8490e-01,  8.2905e-01,\n",
      "         1.9803e-01, -7.3324e-01, -5.5246e-01, -4.6930e-01, -1.7326e-01,\n",
      "         2.5662e-01, -1.0521e+00, -3.2560e-01,  1.3647e-01,  1.2832e-01,\n",
      "        -2.4086e+00, -2.8245e-01,  7.1802e-01,  8.7841e-01,  7.1353e-02,\n",
      "         5.3584e-01,  5.7417e-01,  5.6343e-01, -1.2018e-01,  5.0456e-01,\n",
      "        -6.1081e-01,  4.9771e-01,  1.2290e+00,  7.7136e-01,  1.2948e+00,\n",
      "         7.4913e-01,  1.6564e-01, -2.8906e-01, -5.0207e-01,  3.1034e-01,\n",
      "        -8.0094e-01, -7.7947e-01,  2.1794e-01, -1.0672e+00, -2.2227e-01,\n",
      "         3.4507e-01, -1.3336e-01, -5.8961e-02,  2.1234e-01, -1.4277e+00,\n",
      "         7.4573e-02, -3.1233e-01, -5.3944e-04,  1.5698e-01, -1.0382e+00,\n",
      "         9.8328e-01, -5.9559e-01,  4.3997e-01,  5.5026e-01,  3.6462e-01,\n",
      "         8.4136e-01, -1.4328e-03, -2.4872e-01, -1.0055e-01,  9.5509e-02])\n"
     ]
    }
   ],
   "source": [
    "i = 50\n",
    "word = train_data.iloc[i][\"word/symbol\"]\n",
    "print(f\"word at position {i}: {word}\")\n",
    "print(f'embedding: \\n {glove[word]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF6zaZQl2aDd"
   },
   "source": [
    "## Useful classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpf8ylTl2eMz"
   },
   "source": [
    "### Dataset\n",
    "it is a usefull class that helps creating a dataloader which is very usefull for training a network since it automatically manages batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "SDdt_6ytPtLc"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsNOjTC02-qZ"
   },
   "source": [
    "### Vocabulary\n",
    "It is a class that really helps with managing the words in our dataset. it creates 3 structures:\n",
    "  - `word2idx` which is a dictionary that maps every word to the corresponding token.\n",
    "  - `idx2word` which is a list that works as the inverse function to `word2idx` mapping back every token to the corresponding word.\n",
    "  - `vectors` which is a list that maps every token to the corresponding embedding vector. If no embedding vector has been given for the corresponding word, the vectors list will return a 0 tensor.\n",
    "\n",
    "In addition, the `length` variable will contains the length of the embedding vectors and `dim` will contains the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5ovAQ8a33uFU"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "  \"\"\"\n",
    "  A class containing all the words used in the training.\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  word2idx : Dict\n",
    "    Maps every word to the corresponding token.\n",
    "  idx2word : List[str]\n",
    "    Works as the inverse function to `word2idx` mapping back every token to the corresponding word.\n",
    "  vectors : list[torch.Tensor]\n",
    "    maps every token to the corresponding embedding vector. If no embedding vector has been given for the corresponding word, the vectors list will return a 0 tensor.\n",
    "  length : int\n",
    "    contains the length of the embedding vectors.\n",
    "  dim : int\n",
    "    contains the size of the vocabulary.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               words:'list[str]',\n",
    "               pretrained_vectors: 'torchtext.vocab.Vectors' = None,\n",
    "               specials:'list[str]' = ['<unk>', '<pad>'],\n",
    "               vectors_length:'int' = -1) -> None:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: list[str]\n",
    "      The unique words contained in the vocabulary\n",
    "    pretrained_vectors: torchtext.vocab.Vectors\n",
    "      the pretrained embedding containing the Tensors that will be used for the embedding. If there are words included in this class not included in the `words` list they will be added to the vocabulary too.\n",
    "      It can be None but, in this case, a 0 Tensor will be created for the embedding. In the case this value is None, the `vectors_length` is mandatory.\n",
    "    specials: list[str]\n",
    "      A list that contains the special tokens that will be added to the vocabulary. This token will be the first tokens in the resulting list.\n",
    "    vectors_length: int\n",
    "      This parameter is mandatory only if the `pretrained_vectors` parameter is None. It represent the length of each Tensor used in the embedding.\n",
    "    \"\"\"\n",
    "    self.__word2idx = {}\n",
    "    self.idx2word = []\n",
    "    self.vectors = []\n",
    "\n",
    "    self.pre_trained = pretrained_vectors != None\n",
    "\n",
    "    pre_keys = []\n",
    "    pre_vectors = {}\n",
    "\n",
    "    if self.pre_trained:\n",
    "      pre_keys = pretrained_vectors.stoi.keys()\n",
    "      pre_vectors = pretrained_vectors\n",
    "\n",
    "    self.length = vectors_length\n",
    "    if self.pre_trained:\n",
    "      self.length = len(pretrained_vectors.vectors[0])\n",
    "      if vectors_length != -1 and self.length != vectors_length:\n",
    "        raise Exception(f\"vectors_length {vectors_length} incompatible with length of pretrained_vectors {self.length}. Consider removing the vector length property\")\n",
    "    if self.length == -1:\n",
    "      raise Exception(\"either a the pretrained_vectors or the vectors_length properties should be provided\")\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for word in specials:\n",
    "      self.__add_word(word, idx, pre_keys, pre_vectors)\n",
    "      idx += 1\n",
    "\n",
    "    for word in pre_keys:\n",
    "      self.__add_word(word, idx, pre_keys, pre_vectors)\n",
    "      idx += 1\n",
    "\n",
    "    for word in words:\n",
    "      if not word in self.__word2idx:\n",
    "        self.__add_word(word, idx, pre_keys, pre_vectors)\n",
    "        idx += 1\n",
    "\n",
    "    self.dim = idx\n",
    "    self.vectors = torch.stack(self.vectors)\n",
    "\n",
    "  def word2idx(self, word):\n",
    "    if word in self.__word2idx:\n",
    "      return self.__word2idx[word]\n",
    "    return self.__word2idx['<unk>']\n",
    "\n",
    "  def __add_word(self, word:'str', idx:'int', pre_keys:'list', pre_vectors: 'dict') -> None:\n",
    "      self.__word2idx[word] = idx\n",
    "      self.idx2word.append(word)\n",
    "      self.vectors.append(pre_vectors[word] if word in pre_keys else zeros(self.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ujVbEWbDVTk_"
   },
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(np.unique(train_data['word/symbol']), glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeOkToIgXNdA",
    "outputId": "90461b15-6dff-487f-b32b-957104411ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13077\n",
      "hello\n",
      "tensor([ 0.2669,  0.3963,  0.6169, -0.7745, -0.1039,  0.2670,  0.2788,  0.3099,\n",
      "         0.0055, -0.0853,  0.7360, -0.0984,  0.5479, -0.0303,  0.3348,  0.1409,\n",
      "        -0.0070,  0.3257,  0.2290,  0.4656, -0.1953,  0.3749, -0.7139, -0.5178,\n",
      "         0.7704,  1.0881, -0.6601, -0.1623,  0.9119,  0.2105,  0.0475,  1.0019,\n",
      "         1.1133,  0.7009, -0.0870,  0.4757,  0.1636, -0.4447,  0.4469, -0.9382,\n",
      "         0.0131,  0.0860, -0.6746,  0.4966, -0.0378, -0.1104, -0.2861,  0.0746,\n",
      "        -0.3153, -0.0938, -0.5707,  0.6686,  0.4531, -0.3415, -0.7166, -0.7527,\n",
      "         0.0752,  0.5790, -0.1191, -0.1138, -0.1003,  0.7134, -1.1574, -0.7403,\n",
      "         0.4045,  0.1802,  0.2145,  0.3764,  0.1124, -0.5364, -0.0251,  0.3189,\n",
      "        -0.2501, -0.6328, -0.0118,  1.3770,  0.8601,  0.2048, -0.3681, -0.6887,\n",
      "         0.5351, -0.4656,  0.2739,  0.4118, -0.8540, -0.0463,  0.1130, -0.2733,\n",
      "         0.1564, -0.2033,  0.5359,  0.5978,  0.6047,  0.1373,  0.4223, -0.6128,\n",
      "        -0.3849,  0.3584, -0.4846,  0.3073])\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary.word2idx('hello'))\n",
    "print(vocabulary.idx2word[13077])\n",
    "print(vocabulary.vectors[13077])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_CHOUNA8oMC"
   },
   "source": [
    "## Data pre-processing and preparation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9_H2yulB7Z0"
   },
   "source": [
    "### PosEncoding\n",
    "It is a similar class to Vocabulary but for the position. It works basically the same but with the one-hot-encoding instead of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "AXhmgSD19yGf"
   },
   "outputs": [],
   "source": [
    "class PosEncoding:\n",
    "  \"\"\"\n",
    "  A class containing all the pos values used in the training.\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  pos2idx : Dict\n",
    "    Maps every pos to the corresponding token.\n",
    "  idx2pos : List[str]\n",
    "    Works as the inverse function to `pos2idx` mapping back every token to the corresponding pos.\n",
    "  encoding : list[torch.Tensor]\n",
    "    maps every pos to the corresponding one-hot-encoded vector.\n",
    "  dim : int\n",
    "    contains the size of the vocabulary.\n",
    "  \"\"\"\n",
    "  def __init__(self, pos_labels: 'list[str]', specials:'list[str]' = ['<pad>']) -> None:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_labels: list[str]\n",
    "      The unique words contained in the vocabulary\n",
    "    specials: list[str]\n",
    "      A list that contains the special pos that will be added to the vocabulary.\n",
    "    \"\"\"\n",
    "    self.pos2idx = {}\n",
    "    self.idx2pos = []\n",
    "    self.encoding = []\n",
    "    starting_tensor = [0 for _ in range(len(pos_labels) + len(specials))]\n",
    "    idx = 0\n",
    "\n",
    "    for pos in specials:\n",
    "      self.__add_pos(pos, idx, starting_tensor)\n",
    "      idx += 1\n",
    "\n",
    "    for pos in pos_labels:\n",
    "      self.__add_pos(pos, idx, starting_tensor)\n",
    "      idx += 1\n",
    "\n",
    "\n",
    "  def __add_pos(self, pos:'str', idx:'int', starting_tensor:'torch.Tensor'):\n",
    "    self.pos2idx[pos] = idx\n",
    "    self.idx2pos.append(pos)\n",
    "    self.encoding.append(starting_tensor.copy())\n",
    "    self.encoding[idx][idx] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4KVceH88sfz"
   },
   "source": [
    "### Y encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGudSj4L8yS5",
    "outputId": "d38637eb-7c64-441b-eb4d-4d0755748c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 45 unique pos label values: \n",
      "- #\n",
      "- $\n",
      "- ''\n",
      "- ,\n",
      "- -LRB-\n",
      "- -RRB-\n",
      "- .\n",
      "- :\n",
      "- CC\n",
      "- CD\n",
      "- DT\n",
      "- EX\n",
      "- FW\n",
      "- IN\n",
      "- JJ\n",
      "- JJR\n",
      "- JJS\n",
      "- LS\n",
      "- MD\n",
      "- NN\n",
      "- NNP\n",
      "- NNPS\n",
      "- NNS\n",
      "- PDT\n",
      "- POS\n",
      "- PRP\n",
      "- PRP$\n",
      "- RB\n",
      "- RBR\n",
      "- RBS\n",
      "- RP\n",
      "- SYM\n",
      "- TO\n",
      "- UH\n",
      "- VB\n",
      "- VBD\n",
      "- VBG\n",
      "- VBN\n",
      "- VBP\n",
      "- VBZ\n",
      "- WDT\n",
      "- WP\n",
      "- WP$\n",
      "- WRB\n",
      "- ``\n"
     ]
    }
   ],
   "source": [
    "pos_labels = np.unique(train_data['pos label'])\n",
    "number_of_pos_labels = len(pos_labels)\n",
    "print(f'there are {number_of_pos_labels} unique pos label values: ' + \"\\n- \" + \"\\n- \".join(pos_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8eRA4uJaCLyC"
   },
   "outputs": [],
   "source": [
    "pos_encoding = PosEncoding(pos_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iu9QGLrlCiYt",
    "outputId": "8cf659af-8232-4844-e130-cec9e52348b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "NNP\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(pos_encoding.pos2idx['NNP'])\n",
    "print(pos_encoding.idx2pos[21])\n",
    "print(pos_encoding.encoding[21])\n",
    "print(pos_encoding.encoding[21][21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "5gXfKc2TJ-pd"
   },
   "outputs": [],
   "source": [
    "def transform_data(input_data:'pd.Dataframe', vocab:'Vocabulary', encoding:'PosEncoding', sentence_split:'bool'=True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "  \"\"\"\n",
    "    A function that returns a tuple containing two Tensor, the first containg the tokenized words of the dataframe, the second containing the encoded labels of the dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data: pd.Dataframe\n",
    "      The dataframe containing the data to transform. The value of the words will be checked in the 'word/symbol' column while the label will be cheked in the pos label one. If the sentence_split parameter is false, the elements will be grouped by the source file column.\n",
    "    vocab: Vocabulary\n",
    "      The vocabulary used to tokenize the words.\n",
    "    encoding: PosEncoding\n",
    "      The encoding of the labels.\n",
    "    sentence_split: bool\n",
    "      It tells how to split the sentences: if tru, they will be split by the '.' token, if false they will be split by the source file.\n",
    "  \"\"\"\n",
    "  if sentence_split:\n",
    "    return transform_data_by_sentences(input_data, vocab, encoding)\n",
    "  return transform_data_by_files(input_data, vocab, encoding)\n",
    "\n",
    "def tail_pad(x, tot_len, pad_element):\n",
    "  l = len(x)\n",
    "  return x + [pad_element for _ in range(tot_len - l)]\n",
    "\n",
    "def transform_data_by_sentences(input_data:'pd.Dataframe', vocab:'Vocabulary', encoding:'PosEncoding') -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "  out_lists_x = [[]]\n",
    "  out_lists_y = [[]]\n",
    "  idx = 0\n",
    "  for i in range(input_data.shape[0]):\n",
    "    row = input_data.iloc[i, :]\n",
    "    out_lists_x[idx].append(row['word/symbol'])\n",
    "    out_lists_y[idx].append(row['pos label'])\n",
    "    if row['word/symbol'] == '.':\n",
    "      out_lists_x.append([])\n",
    "      out_lists_y.append([])\n",
    "      idx += 1\n",
    "  max_len = 0\n",
    "  for sentence in out_lists_x:\n",
    "    if len(sentence) > max_len:\n",
    "      max_len = len(sentence)\n",
    "  for i in range(len(out_lists_x)):\n",
    "    out_lists_x[i] = tail_pad(out_lists_x[i], max_len, '<pad>')\n",
    "    out_lists_y[i] = tail_pad(out_lists_y[i], max_len, '<pad>')\n",
    "    token_words = []\n",
    "    for word in out_lists_x[i]:\n",
    "      token_words.append(vocab.word2idx(word))\n",
    "    encoded_y = []\n",
    "    for pos in out_lists_y[i]:\n",
    "      encoded_y.append(encoding.encoding[encoding.pos2idx[pos]])\n",
    "    out_lists_x[i] = token_words\n",
    "    out_lists_y[i] = encoded_y\n",
    "  return torch.LongTensor(out_lists_x), torch.Tensor(out_lists_y)\n",
    "\n",
    "def transform_data_by_files(input_data:'pd.Dataframe', vocab:'Vocabulary', encoding:'PosEncoding') -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "  out_lists_x = {}\n",
    "  out_lists_y = {}\n",
    "  for i in range(input_data.shape[0]):\n",
    "    row = input_data.iloc[i, :]\n",
    "    source = row['source file']\n",
    "    if not source in out_lists_x:\n",
    "      out_lists_x[source] = []\n",
    "      out_lists_y[source] = []\n",
    "    out_lists_x[source].append(row['word/symbol'])\n",
    "    out_lists_y[source].append(row['pos label'])\n",
    "  max_len = 0\n",
    "  for key in out_lists_x.keys():\n",
    "    if len(out_lists_x[key]) > max_len:\n",
    "      max_len = len(out_lists_x[key])\n",
    "  for key in out_lists_x.keys():\n",
    "    out_lists_x[key] = tail_pad(out_lists_x[key], max_len, '<pad>')\n",
    "    out_lists_y[key] = tail_pad(out_lists_y[key], max_len, '<pad>')\n",
    "    token_words = []\n",
    "    for word in out_lists_x[key]:\n",
    "      token_words.append(vocab.word2idx(word))\n",
    "    encoded_y = []\n",
    "    for pos in out_lists_y[key]:\n",
    "      encoded_y.append(encoding.encoding[encoding.pos2idx[pos]])\n",
    "    out_lists_x[key] = token_words\n",
    "    out_lists_y[key] = encoded_y\n",
    "  return torch.LongTensor([ out_lists_x[key] for key in out_lists_x.keys()]), torch.Tensor([out_lists_y[key] for key in out_lists_y.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "06j2xmDpQU3n"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = transform_data(train_data, vocabulary, pos_encoding)\n",
    "x_validation, y_validation = transform_data(validation_data, vocabulary, pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bJHoUF-PWgx"
   },
   "source": [
    "# [Task 3 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_bPdqU-PWg0"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6gHqF1O9xYd",
    "outputId": "0ee21c5f-bf3b-4d8f-a351-0a4d1c3ed448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: %s' % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyZr-MXqE2pZ"
   },
   "source": [
    "## LSTM Layer\n",
    "\n",
    "A very simple layer. It creates a LSTM that can be used with the `NeuralNetwork` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "XmlrTkavE68H"
   },
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  A very simple layer. It creates a LSTM that can be used with the `NeuralNetwork` class.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size:'int', hidden_size:'int', bidirectional:'bool') -> None:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size: int\n",
    "      The size of the input to the LSTM layer.\n",
    "    hidden_size: int\n",
    "      The number of LSTM layers.\n",
    "    bidirectional: bool\n",
    "      If the LSTM layer are birectional or not.\n",
    "    \"\"\"\n",
    "    super(LSTMLayer, self).__init__()\n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out, _ = self.lstm(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmEFLqd3FITE"
   },
   "source": [
    "## Embed Layer\n",
    "\n",
    "This layer handles the embedding of the tokens. It contains two embedding layers: the first assumes that there are pretrained vectors to use \\(vector given as arguments to the `__init__` function\\), the second one covers the values that are not present in the pretrained vector. The second embedding layer will be initialized with random values.\n",
    "\n",
    "The first embedding layer can be frozen in order to avoid training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKNXUbVdGwjt"
   },
   "source": [
    "### How the replace Embedding layer works\n",
    "![img](https://drive.google.com/uc?export=view&id=1tC7yKZwKDkv6RkBK4nU5kz3ckKocnJf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "2_2LSqC0GENZ"
   },
   "outputs": [],
   "source": [
    "class EmbedLayer_replace(nn.Module):\n",
    "  def __init__(self, vocabulary, embedding_dim):\n",
    "        super(EmbedLayer_replace, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocabulary.vectors)\n",
    "        self.oov_embedding = nn.Embedding(vocabulary.dim, embedding_dim)\n",
    "        nn.init.uniform_(self.oov_embedding.weight, -1.0, 1.0)\n",
    "\n",
    "  def freeze(self, freeze:'bool'):\n",
    "    self.embedding.freeze = freeze\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    embedded = self.embedding(x)\n",
    "    mask = (embedded[:, :, -1] == 0).nonzero()\n",
    "\n",
    "    embedded[mask[:, 0], mask[:, 1], :] = self.oov_embedding(x[mask[:, 0], mask[:, 1]])\n",
    "    return embedded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duNiM5h_gXoy"
   },
   "source": [
    "This second version of the embedding works a bit differently: It sums the two embedding values instead of replacing the oov words only. The second random embedding should \\(hopefully\\) work as \"fine-tuning\" layer for training words and be the only embedding for non-pretrained words. We should try and see which version works the best  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAs47oOXX_Cc"
   },
   "source": [
    "### How the sum Embedding layer works\n",
    "![img](https://drive.google.com/uc?export=view&id=1RrF4tv5YD7gNgjzTJXpeUq3LgQQGAAyH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "1fs__rBUgBAR"
   },
   "outputs": [],
   "source": [
    "class EmbedLayer_sum(nn.Module):\n",
    "  def __init__(self, vocabulary, embedding_dim):\n",
    "        super(EmbedLayer_sum, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocabulary.vectors)\n",
    "        self.oov_embedding = nn.Embedding(vocabulary.dim, embedding_dim)\n",
    "        nn.init.uniform_(self.oov_embedding.weight, -1.0, 1.0)\n",
    "\n",
    "  def freeze(self, freeze:'bool'):\n",
    "    self.embedding.freeze = freeze\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    pre_trained_embedded = self.embedding(x)\n",
    "\n",
    "    oov_embeds = self.oov_embedding(x)\n",
    "    return pre_trained_embedded + oov_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CApiISQ1D18z"
   },
   "source": [
    "## NeuralNetwork\n",
    "This class is a really simple class that helps with crating a neural network. The constructor needs the optimizer and the loss that are gonna be useful for for training and the device on which the network will be trained. Each parameter can be updated in a second moment.\n",
    "### Methods\n",
    " - The add method can be used to add layers to the nn.\n",
    " - The compile method can be used to actually create the nn \\(the order of the layers will be the same as the order they have been passed to the add method\\)\n",
    " - The train method can be used to train the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HO_qvV3Zwm_Q",
    "outputId": "b8f095e1-11bb-49f0-c95b-02a2cb4c78ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "8\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(pos_encoding.pos2idx['#'])\n",
    "print(pos_encoding.pos2idx['$'])\n",
    "print(pos_encoding.pos2idx[\"''\"])\n",
    "print(pos_encoding.pos2idx[','])\n",
    "print(pos_encoding.pos2idx['.'])\n",
    "print(pos_encoding.pos2idx[':'])\n",
    "print(pos_encoding.pos2idx['``'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "0shqASbzk0V2"
   },
   "outputs": [],
   "source": [
    "def masked_accuracy(best_guesses, targets):\n",
    "\n",
    "  exclude_values = torch.Tensor([pos_encoding.pos2idx['<pad>'],\n",
    "                    pos_encoding.pos2idx['#'],\n",
    "                    pos_encoding.pos2idx['$'],\n",
    "                    pos_encoding.pos2idx[\"''\"],\n",
    "                    pos_encoding.pos2idx[','],\n",
    "                    pos_encoding.pos2idx['.'],\n",
    "                    pos_encoding.pos2idx[':'],\n",
    "                    pos_encoding.pos2idx['``']])\n",
    "  idx_to_use = torch.isin(targets, exclude_values, invert=True)\n",
    "  actual_targets = targets[idx_to_use]\n",
    "  actual_best_guesses = best_guesses[idx_to_use]\n",
    "  correct_percentage = accuracy_score(actual_targets,actual_best_guesses)\n",
    "  return correct_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "0gRFP9OJ6mWn"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  \"\"\"\n",
    "  This class implements a simple interface to get a working neural network using pytorch.\n",
    "  \"\"\"\n",
    "  def __init__(self, optimizer = torch.optim.Adam, loss = nn.CrossEntropyLoss(), device:'str' = 'cpu'):\n",
    "      \"\"\"\n",
    "      Parameters\n",
    "      ----------\n",
    "      optimizer:\n",
    "        The optimizer to use while training, default to Adam.\n",
    "      loss:\n",
    "        The loss function to use while training, default to crossentropy\n",
    "      device: str\n",
    "        The device where to train end evaluate the neural network. It must be either cpu or a valid pytorch device (e.g. cuda). Default to cpu\n",
    "      \"\"\"\n",
    "      self.layers = []\n",
    "      self.net = None\n",
    "      self.optimizer = optimizer\n",
    "      self.loss = loss\n",
    "      self.device = device\n",
    "\n",
    "  def add(self, *layer:'nn.Module'):\n",
    "    \"\"\"\n",
    "      A function to add layers to the Neural network. The layers must be presentend in the order you want them to be called.\n",
    "      Parameters\n",
    "      ----------\n",
    "      layer: nn.Module\n",
    "        An arbitrary amount of layers to be added to the Neural network. The layers must be presentend in the order you want them to be called.\n",
    "    \"\"\"\n",
    "    if not self.net is None:\n",
    "      print(\"WARNING: the neural network has already been built. If you want the added layer to be added to the built network please rebuild it.\")\n",
    "    self.layers += layer\n",
    "\n",
    "  def compile(self):\n",
    "    \"\"\"\n",
    "      Call this method before using the Neural network for inference or training. It builds the actual neural network.\n",
    "    \"\"\"\n",
    "    if not self.net is None:\n",
    "      print(\"WARNING: the previous network will be discarded. Please retrain the network before using it for inference.\")\n",
    "    self.net = nn.Sequential(*self.layers)\n",
    "    self.net = self.net.to(self.device)\n",
    "    return self\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "     return f\"{self.net}\"\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "  def train(self,\n",
    "            train_loader:'torch.utils.data.DataLoader',\n",
    "            validation_loader:'torch.utils.data.DataLoader',\n",
    "            learning_rate:'float'=.1,\n",
    "            epochs:'int'=10,\n",
    "            metrics:'dict[str,callable]' = {'accuracy': masked_accuracy}) -> Tuple[dict[str,list[float]],dict[str,list[float]]]:\n",
    "    \"\"\"\n",
    "      A simple training loop for the neural network. It returns the epochs loss and accuracy history both on the training and the validation set. The tuple will be formatted as:\n",
    "      train loss, train accuracy, val loss, val accuracy\n",
    "      Parameters\n",
    "      ----------\n",
    "      train_loader: torch.utils.data.DataLoader\n",
    "        A dataloader containing the dataset that will be used for training the network\n",
    "      validation_loader: torch.utils.data.DataLoader\n",
    "        A dataloader containing the dataset that will be used for validate the network at the end of each epoch\n",
    "      learning_rate: float\n",
    "        The learning rate that will be used in the optimizer to train the network. Default to .1\n",
    "      epochs:\n",
    "        The number of training epochs, default to 10.\n",
    "    \"\"\"\n",
    "    net = self.net\n",
    "    optimizer = self.optimizer(net.parameters(), learning_rate)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    start_ts = time.time()\n",
    "\n",
    "    total_batch = int(len(train_loader.dataset) / train_loader.batch_size)\n",
    "    train_metrics_scores = {}\n",
    "    val_metrics_scores = {}\n",
    "    for key in metrics:\n",
    "        train_metrics_scores[key] = []\n",
    "        val_metrics_scores[key] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            n_batch, n_sequence, n_classes = outputs.shape\n",
    "            loss_outputs = torch.reshape(outputs, (n_batch, n_classes, n_sequence))\n",
    "            loss_labels = torch.reshape(labels, (n_batch, n_classes, n_sequence))\n",
    "            loss = self.loss(loss_outputs, loss_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            predicted_classes = torch.max(outputs, len(outputs.shape) - 1)[1].view(-1).cpu()\n",
    "            normal_labels = torch.max(labels, len(labels.shape) - 1)[1].view(-1).cpu()\n",
    "\n",
    "            stdout.write(f\"\\rbatch {batch_idx + 1}/{total_batch} ----- loss: {loss.cpu()} ----- {'-----'.join([f'{key}: {metrics[key](predicted_classes, normal_labels)}' for key in metrics.keys()])}\")\n",
    "            stdout.flush()\n",
    "\n",
    "        val_metrics, val_loss = self.__validate(validation_loader, metrics)\n",
    "        for key in metrics:\n",
    "          val_metrics_scores[key].append(val_metrics[key])\n",
    "        val_loss_history.append(val_loss)\n",
    "        train_metrics, train_loss = self.__validate(train_loader, metrics)\n",
    "        for key in metrics:\n",
    "          train_metrics_scores[key].append(train_metrics[key])\n",
    "        train_loss_history.append(train_loss)\n",
    "        out_str = \"======================================================================================================================================\\n\" + \\\n",
    "        f\"EPOCH {epoch + 1} training loss: {train_loss_history[-1]} - validation loss: {val_loss_history[-1]}\\n\" + \\\n",
    "        '\\n'.join([f\"EPOCH {epoch + 1} training {metric}: {train_metrics_scores[metric][-1]} - validation {metric}: {val_metrics_scores[metric][-1]}\" for metric in metrics.keys()]) + \\\n",
    "        \"\"\"\n",
    "======================================================================================================================================\n",
    "        \"\"\"\n",
    "        stdout.write(\"\\r\" + \" \" * len(out_str) + \"\\r\")\n",
    "        stdout.flush()\n",
    "        stdout.write(out_str)\n",
    "        stdout.flush()\n",
    "        print()\n",
    "        stdout.write(f'losses: {train_loss_history} \\nscores: {val_loss_history}')\n",
    "        stdout.flush()\n",
    "        print()\n",
    "        \n",
    "    train_metrics_scores['loss'] = train_loss_history\n",
    "    val_metrics_scores['loss'] = val_loss_history\n",
    "    return train_metrics_scores, val_metrics_scores\n",
    "\n",
    "  def __validate(self, loader, metrics):\n",
    "    losses = []\n",
    "    metrics_scores = {}\n",
    "    for key in metrics.keys():\n",
    "      metrics_scores[key] = []\n",
    "    net = self.net\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(loader):\n",
    "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            n_batch, n_sequence, n_classes = outputs.shape\n",
    "            outputs = torch.reshape(outputs, (n_batch, n_classes, n_sequence))\n",
    "            labels = torch.reshape(labels, (n_batch, n_classes, n_sequence))\n",
    "            loss = self.loss(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            outputs = torch.reshape(outputs, (n_batch, n_sequence, n_classes))\n",
    "            labels = torch.reshape(labels, (n_batch, n_sequence, n_classes))\n",
    "            predicted_classes = torch.max(outputs, len(outputs.shape) - 1)[1].view(-1)\n",
    "            for key in metrics.keys():\n",
    "              metrics_scores[key].append(metrics[key](predicted_classes.cpu(), torch.max(labels, len(labels.shape) - 1)[1].view(-1).cpu()))\n",
    "\n",
    "    average_loss = sum(losses)/(batch_idx+1)\n",
    "    mean_metrics_scores = {}\n",
    "    for key in metrics.keys():\n",
    "      mean_metrics_scores[key] = sum(metrics_scores[key])/len(loader)\n",
    "    return mean_metrics_scores, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "qyCEDeUSYuxL"
   },
   "outputs": [],
   "source": [
    "HIDDEN_LSTM_SIZE = 500\n",
    "LINEAR_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "I8ExCxvZ1ipS"
   },
   "outputs": [],
   "source": [
    "def compute_frequecies():\n",
    "  frequencies = [train_data[train_data['pos label'] == name]['pos label'].count() for name in pos_encoding.pos2idx.keys()]\n",
    "  max_freq = max(frequencies)\n",
    "  min_freq = min(frequencies)\n",
    "  norm_factor = max_freq - min_freq\n",
    "  weights = [1 + ((max_freq - freq)/norm_factor)**3 for freq in frequencies]\n",
    "  weights[pos_encoding.pos2idx['<pad>']] = 0\n",
    "  return torch.Tensor(weights)\n",
    "weights = compute_frequecies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxduiNiFD14l",
    "outputId": "6b5d47ef-6870-467d-a175-c0d8862de1ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): EmbedLayer_replace(\n",
      "    (embedding): Embedding(402348, 100)\n",
      "    (oov_embedding): Embedding(402348, 100)\n",
      "  )\n",
      "  (1): LSTMLayer(\n",
      "    (lstm): LSTM(100, 500, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (2): Linear(in_features=1000, out_features=46, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_layer = EmbedLayer_replace(vocabulary, 100)\n",
    "embed_layer.freeze(True)\n",
    "baseline = NeuralNetwork(device=device, loss=nn.CrossEntropyLoss(weights.to(device)))\n",
    "baseline.add(embed_layer,\n",
    "          LSTMLayer(100, HIDDEN_LSTM_SIZE, True),\n",
    "          nn.Linear(HIDDEN_LSTM_SIZE * 2, y_train.shape[2])\n",
    "          )\n",
    "print(baseline.compile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uUh_OdCLmmI",
    "outputId": "33bb9cd1-6d0a-4617-9a8b-9aa0e1674878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): EmbedLayer_replace(\n",
      "    (embedding): Embedding(402348, 100)\n",
      "    (oov_embedding): Embedding(402348, 100)\n",
      "  )\n",
      "  (1): LSTMLayer(\n",
      "    (lstm): LSTM(100, 500, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (2): LSTMLayer(\n",
      "    (lstm): LSTM(1000, 500, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (3): Linear(in_features=1000, out_features=46, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_layer = EmbedLayer_replace(vocabulary, 100)\n",
    "embed_layer.freeze(True)\n",
    "Model1 = NeuralNetwork()\n",
    "Model1.add(embed_layer,\n",
    "          LSTMLayer(100, HIDDEN_LSTM_SIZE, True),\n",
    "          LSTMLayer(HIDDEN_LSTM_SIZE * 2, HIDDEN_LSTM_SIZE, True),\n",
    "          nn.Linear(HIDDEN_LSTM_SIZE * 2, y_train.shape[2]))\n",
    "print(Model1.compile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCk7QX0vL77t",
    "outputId": "e1aba773-f8c7-4b2e-dfbc-7becb78b1657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): EmbedLayer_replace(\n",
      "    (embedding): Embedding(402348, 100)\n",
      "    (oov_embedding): Embedding(402348, 100)\n",
      "  )\n",
      "  (1): LSTMLayer(\n",
      "    (lstm): LSTM(100, 500, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (2): Linear(in_features=1000, out_features=200, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=200, out_features=46, bias=True)\n",
      "  (5): Softmax(dim=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_layer = EmbedLayer_replace(vocabulary, 100)\n",
    "embed_layer.freeze(True)\n",
    "model2 = NeuralNetwork()\n",
    "model2.add(embed_layer,\n",
    "          LSTMLayer(100, HIDDEN_LSTM_SIZE, True),\n",
    "          nn.Linear(HIDDEN_LSTM_SIZE * 2, LINEAR_SIZE),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(LINEAR_SIZE, y_train.shape[2]),\n",
    "          nn.Softmax(dim=0))\n",
    "print(model2.compile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "okAVljr7ILOM"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "train_dataset = Dataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "validation_dataset = Dataset(x_validation, y_validation)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz3veASQEX1s",
    "outputId": "0f73528c-2d19-4612-88ae-25c2ca2d276b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================                                                                                                                                                                                                                                                                                                                         \n",
      "EPOCH 1 training loss: 1.4607067108154297 - validation loss: 2.450054883956909\n",
      "EPOCH 1 training accuracy: 0.10776457324155474 - validation accuracy: 0.08935782508465206\n",
      "======================================================================================================================================\n",
      "        \n",
      "losses: [tensor(1.4607)] \n",
      "scores: [tensor(2.4501)]\n",
      "======================================================================================================================================                                                                                                                                                                                                                                                                                                                        \n",
      "EPOCH 2 training loss: 1.2597166299819946 - validation loss: 1.7760167121887207\n",
      "EPOCH 2 training accuracy: 0.2007188635010148 - validation accuracy: 0.1820726282772384\n",
      "======================================================================================================================================\n",
      "        \n",
      "losses: [tensor(1.4607), tensor(1.2597)] \n",
      "scores: [tensor(2.4501), tensor(1.7760)]\n"
     ]
    }
   ],
   "source": [
    "r = baseline.train(train_loader, validation_loader, epochs=2, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYMHZEeMPWg1"
   },
   "source": [
    "# [Task 4 - 1.0 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0AFgk7qPWg2"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRbAjydQPWg2"
   },
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_embedding(embedding_length):\n",
    "    return [i for i in range(embedding_length)]\n",
    "\n",
    "def generate_predictions(model, test_set):\n",
    "    predictions = model.predict(test_set)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def masked_f1_score(predicted_values, targets):\n",
    "    f1_class_scores = dict()\n",
    "    exclude_values = torch.Tensor([pos_encoding.pos2idx['<pad>'],\n",
    "                    pos_encoding.pos2idx['#'],\n",
    "                    pos_encoding.pos2idx['$'],\n",
    "                    pos_encoding.pos2idx[\"''\"],\n",
    "                    pos_encoding.pos2idx[','],\n",
    "                    pos_encoding.pos2idx['.'],\n",
    "                    pos_encoding.pos2idx[':'],\n",
    "                    pos_encoding.pos2idx['``']])\n",
    "    idx_to_use = torch.isin(targets, exclude_values, invert=True)\n",
    "    actual_targets = targets[idx_to_use]\n",
    "    actual_predictions = predicted_values[idx_to_use]\n",
    "    classes = set(actual_targets.detach().numpy().tolist() + actual_predictions.detach().numpy().tolist())\n",
    "    \n",
    "    for cl in classes:\n",
    "        true_positive, false_positive, false_negative = 0, 0, 0\n",
    "        for act_targ, act_pred in zip(actual_targets, actual_predictions):\n",
    "            if act_pred == cl:\n",
    "                if act_pred == act_targ:\n",
    "                    true_positive += 1\n",
    "                else:\n",
    "                    false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "            score = true_positive / (true_positive + (false_positive + false_negative))\n",
    "            f1_class_scores[cl] = score\n",
    "    \n",
    "    return np.mean(np.array(list(f1_class_scores.values())))\n",
    "    # correct_percentage = f1_score(actual_targets, actual_predictions, average='macro')\n",
    "    # return correct_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rt8yXVMPWg2"
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1SoEvCkPWg3"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================                                                                                                                                                                                                                                                                                                                           \n",
      "EPOCH 1 training loss: 1.1324265003204346 - validation loss: 1.5818884372711182\n",
      "EPOCH 1 training f1_score: 0.018294107050929143 - validation f1_score: 0.01802206510099769\n",
      "======================================================================================================================================\n",
      "        \n",
      "losses: [tensor(1.1324)] \n",
      "scores: [tensor(1.5819)]\n",
      "======================================================================================================================================                                                                                                                                                                                                                                                                                                                            \n",
      "EPOCH 2 training loss: 1.0909565687179565 - validation loss: 1.5826140642166138\n",
      "EPOCH 2 training f1_score: 0.023388235129027574 - validation f1_score: 0.022847761582783975\n",
      "======================================================================================================================================\n",
      "        \n",
      "losses: [tensor(1.1324), tensor(1.0910)] \n",
      "scores: [tensor(1.5819), tensor(1.5826)]\n"
     ]
    }
   ],
   "source": [
    "r_f1 = baseline.train(train_loader, validation_loader, epochs=2, learning_rate=0.01, metrics = {'f1_score': masked_f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_DGswu1PWg3"
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to evaluate your best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo7c7KDbPWg4"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible)\n",
    "* Comment the about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(losses, validations, score_name):\n",
    "    train_batch = np.arange(len(losses['loss'])) \n",
    "    train_loss = list(map(int, losses['loss']))\n",
    "    train_score = losses[score_name]\n",
    "    \n",
    "    val_batch = np.arange(len(validations['loss'])) \n",
    "    val_loss = list(map(int, validations['loss']))\n",
    "    val_score = validations[score_name]\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    ax1.plot(train_batch, train_loss, color='blue', label='train_loss')\n",
    "    ax1.plot(train_batch, train_score, color='green', label=f'train_{score_name}')\n",
    "    ax1.set_title('Train')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(val_batch, val_loss, color='blue', label='val_loss')\n",
    "    ax2.plot(val_batch, val_score, color='green', label=f'val_{score_name}')\n",
    "    ax2.set_title('Validation')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results(r[0], r[1], 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results(r_f1[0], r_f1[1], 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_labels = np.unique(test_data['pos label'])\n",
    "test_pos_encoding = PosEncoding(test_pos_labels)\n",
    "x_test, y_test = transform_data(test_data, vocabulary, test_pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = baseline.net(x_test)\n",
    "print(predictions.shape)\n",
    "print(y_test.shape)\n",
    "# padded_y = torch.nn.functional.pad(input=y_test, pad=(0, 5, 0, 0), mode='constant', value=0) # I'm working on it!!!\n",
    "# f1_masked = masked_f1_score(predictions, padded_y)\n",
    "# print(f1_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az6O5TuxPWg4"
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUJ9UKMZPWg5"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkUvb_LkPWg5"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K62xEDwFPWg6"
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "017zL6z1PWg6"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYjOdgbMPWg6"
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUsAIjyYPWg7"
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62oRvXM2PWg7"
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULplO2tTPWg8"
   },
   "source": [
    "### Keras TimeDistributed Dense layer\n",
    "\n",
    "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNEBeS9HPWg8"
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snebFw_VPWg9"
   },
   "source": [
    "### Punctuation\n",
    "\n",
    "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
    "\n",
    "You should **ignore** it during metrics computation.\n",
    "\n",
    "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9sTFDJrPWg9"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
