{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Human Value Detection, Multi-label classification, Transformers, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the [Human Value Detection challenge](https://aclanthology.org/2022.acl-long.306/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "Arguments are paired with their conveyed human values.\n",
    "\n",
    "Arguments are in the form of **premise** $\\rightarrow$ **conclusion**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Premise**: *``fast food should be banned because it is really bad for your health and is costly''*\n",
    "\n",
    "**Conclusion**: *``We should ban fast food''*\n",
    "\n",
    "**Stance**: *in favour of*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"images/human_values.png\" alt=\"human values\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus\n",
    "\n",
    "Check the official page of the challenge [here](https://touche.webis.de/semeval23/touche23-web/).\n",
    "\n",
    "The challenge offers several corpora for evaluation and testing.\n",
    "\n",
    "You are going to work with the standard training, validation, and test splits.\n",
    "\n",
    "#### Arguments\n",
    "* arguments-training.tsv\n",
    "* arguments-validation.tsv\n",
    "* arguments-test.tsv\n",
    "\n",
    "#### Human values\n",
    "* labels-training.tsv\n",
    "* labels-validation.tsv\n",
    "* labels-test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "#### arguments-*.tsv\n",
    "```\n",
    "\n",
    "Argument ID    A01005\n",
    "\n",
    "Conclusion     We should ban fast food\n",
    "\n",
    "Stance         in favor of\n",
    "\n",
    "Premise        fast food should be banned because it is really bad for your health and is costly.\n",
    "```\n",
    "\n",
    "#### labels-*.tsv\n",
    "\n",
    "```\n",
    "Argument ID                A01005\n",
    "\n",
    "Self-direction: thought    0\n",
    "Self-direction: action     0\n",
    "...\n",
    "Universalism: objectivity: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Splits\n",
    "\n",
    "The standard splits contain\n",
    "\n",
    "   * **Train**: 5393 arguments\n",
    "   * **Validation**: 1896 arguments\n",
    "   * **Test**: 1576 arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Annotations\n",
    "\n",
    "In this assignment, you are tasked to address a multi-label classification problem.\n",
    "\n",
    "You are going to consider **level 3** categories:\n",
    "\n",
    "* Openness to change\n",
    "* Self-enhancement\n",
    "* Conversation\n",
    "* Self-transcendence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How to do that?**\n",
    "\n",
    "You have to merge (**logical OR**) annotations of level 2 categories belonging to the same level 3 category.\n",
    "\n",
    "**Pay attention to shared level 2 categories** (e.g., Hedonism). $\\rightarrow$ [see Table 1 in the original paper.](https://aclanthology.org/2022.acl-long.306/)\n",
    "\n",
    "#### Example\n",
    "\n",
    "```\n",
    "Self-direction: thought:    0\n",
    "Self-direction: action:     1\n",
    "Stimulation:                0\n",
    "Hedonism:                   1\n",
    "\n",
    "Openess to change           1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Download** the specificed training, validation, and test files.\n",
    "* **Encode** split files into a pandas.DataFrame object.\n",
    "* For each split, **merge** the arguments and labels dataframes into a single dataframe.\n",
    "* **Merge** level 2 annotations to level 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import  BertTokenizer, BertForSequenceClassification\n",
    "from random import randint\n",
    "from typing import Tuple\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/arguments-training.tsv')\n",
    "argument_training = f.read()\n",
    "f.close()\n",
    "f = open('data/arguments-validation.tsv')\n",
    "argument_validation = f.read()\n",
    "f.close()\n",
    "f = open('data/arguments-test.tsv')\n",
    "argument_test = f.read()\n",
    "f.close()\n",
    "f = open('data/labels-training.tsv')\n",
    "label_training = f.read()\n",
    "f.close()\n",
    "f = open('data/labels-validation.tsv')\n",
    "label_validation = f.read()\n",
    "f.close()\n",
    "f = open('data/labels-test.tsv')\n",
    "label_test = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label(label_elements):\n",
    "    label = {'Openness to change': 0, 'Self-enhancement': 0, 'Conservation': 0, 'Self-transcendence': 0}\n",
    "    for i in range(len(label_elements)):\n",
    "        label_elements[i] = int(label_elements[i])\n",
    "    label['Openness to change'] = min(1,sum(label_elements[:4]))\n",
    "    label['Self-enhancement'] = min(1,sum(label_elements[3:8]))\n",
    "    label['Conservation'] = min(1,sum(label_elements[7:14]))\n",
    "    label['Self-transcendence'] = min(1,sum(label_elements[13:]))\n",
    "    return label\n",
    "\n",
    "\n",
    "def parse_set(set_training_str, set_labels_str):\n",
    "    set_data = {}\n",
    "    for line in set_training_str.split('\\n')[1:]:\n",
    "        if line != '':\n",
    "            split_line = line.split('\\t')\n",
    "            set_data[split_line[0]] = {'Conclusion': split_line[1], 'Stance': split_line[2], 'Premise': split_line[3]}\n",
    "\n",
    "    for line in set_labels_str.split('\\n')[1:]:\n",
    "        if line != '':\n",
    "            split_line = line.split('\\t') \n",
    "            elem = set_data[split_line[0]]\n",
    "            elem.update(parse_label(split_line[1:]))\n",
    "\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for key in set_data.keys():\n",
    "        elem = set_data[key].copy()\n",
    "        elem['Id'] = key\n",
    "        data_list.append(elem)\n",
    "\n",
    "    return pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe = parse_set(argument_training, label_training)\n",
    "validation_dataframe = parse_set(argument_validation, label_validation)\n",
    "test_dataframe = parse_set(argument_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Openness to change</th>\n",
       "      <th>Self-enhancement</th>\n",
       "      <th>Conservation</th>\n",
       "      <th>Self-transcendence</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A01002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A01005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should end the use of economic sanctions</td>\n",
       "      <td>against</td>\n",
       "      <td>sometimes economic sanctions are the only thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A01006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We should abolish capital punishment</td>\n",
       "      <td>against</td>\n",
       "      <td>capital punishment is sometimes the only optio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A01007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We should ban factory farming</td>\n",
       "      <td>against</td>\n",
       "      <td>factory farming allows for the production of c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A01008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Conclusion       Stance  \\\n",
       "0                  We should ban human cloning  in favor of   \n",
       "1                      We should ban fast food  in favor of   \n",
       "2  We should end the use of economic sanctions      against   \n",
       "3         We should abolish capital punishment      against   \n",
       "4                We should ban factory farming      against   \n",
       "\n",
       "                                             Premise  Openness to change  \\\n",
       "0  we should ban human cloning as it will only ca...                   0   \n",
       "1  fast food should be banned because it is reall...                   0   \n",
       "2  sometimes economic sanctions are the only thin...                   0   \n",
       "3  capital punishment is sometimes the only optio...                   0   \n",
       "4  factory farming allows for the production of c...                   0   \n",
       "\n",
       "   Self-enhancement  Conservation  Self-transcendence      Id  \n",
       "0                 0             1                   0  A01002  \n",
       "1                 0             1                   0  A01005  \n",
       "2                 1             1                   0  A01006  \n",
       "3                 0             1                   1  A01007  \n",
       "4                 0             1                   1  A01008  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x['input_ids'][idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = torch.Tensor([\n",
    "    [\n",
    "    training_dataframe.iloc[i]['Openness to change'], \n",
    "    training_dataframe.iloc[i]['Self-enhancement'], \n",
    "    training_dataframe.iloc[i]['Conservation'], \n",
    "    training_dataframe.iloc[i]['Self-transcendence']\n",
    "    ] for i in range(len(training_dataframe))])\n",
    "\n",
    "validation_y = torch.Tensor([\n",
    "    [\n",
    "    training_dataframe.iloc[i]['Openness to change'], \n",
    "    training_dataframe.iloc[i]['Self-enhancement'], \n",
    "    training_dataframe.iloc[i]['Conservation'], \n",
    "    training_dataframe.iloc[i]['Self-transcendence']\n",
    "    ] for i in range(len(validation_dataframe))])\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 2.0 points] Model definition\n",
    "\n",
    "You are tasked to define several neural models for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"images/model_schema.png\" alt=\"model_schema\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a random uniform classifier (an individual classifier per category).\n",
    "* **Baseline**: implement a majority classifier (an individual classifier per category).\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **BERT w/ C**: define a BERT-based classifier that receives an argument **conclusion** as input.\n",
    "* **BERT w/ CP**: add argument **premise** as an additional input.\n",
    "* **BERT w/ CPS**: add argument premise-to-conclusion **stance** as an additional input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_classifier(_input_shape:Tuple, _output_shape:Tuple):\n",
    "    input_len = len(_input_shape)\n",
    "    def random_classifier(input:'np.array|torch.Tensor'):\n",
    "        batch = 1\n",
    "        if len(input.shape) == input_len + 1:\n",
    "            batch = input.shape[input_len] \n",
    "        output_shape = _output_shape + (batch, )\n",
    "        print(output_shape)\n",
    "        output = np.zeros(shape=output_shape)\n",
    "        for i in range(batch):\n",
    "            idxs = [randint(0, max_idx -1) for max_idx in _output_shape] + [i]\n",
    "            output[tuple(idxs)] = 1\n",
    "        return output\n",
    "    return random_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7)\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_random_classifier((3,5), (4,))\n",
    "input = np.ones((3,5,7))\n",
    "print(classifier(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_classifier(_input_shape:Tuple, _output_shape:Tuple, majority:Tuple):\n",
    "    input_len = len(_input_shape)\n",
    "    def majority_classifier(input:'np.array|torch.Tensor'):\n",
    "        batch = 1\n",
    "        if len(input.shape) == input_len + 1:\n",
    "            batch = input.shape[input_len] \n",
    "        output_shape = _output_shape + (batch, )\n",
    "        print(output_shape)\n",
    "        output = np.zeros(shape=output_shape)\n",
    "        for i in range(batch):\n",
    "            idxs = majority + (i, )\n",
    "            output[tuple(idxs)] = 1\n",
    "        return output\n",
    "    return majority_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7)\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_majority_classifier((3,5), (4,), (2, ))\n",
    "input = np.ones((3,5,7))\n",
    "print(classifier(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  \"\"\"\n",
    "  This class implements a simple interface to get a working neural network using pytorch.\n",
    "  \"\"\"\n",
    "  def __init__(self, net:BertForSequenceClassification, optimizer = torch.optim.Adam, device:'str' = 'cpu'):\n",
    "      \"\"\"\n",
    "      Parameters\n",
    "      ----------\n",
    "      optimizer:\n",
    "        The optimizer to use while training, default to Adam.\n",
    "      loss:\n",
    "        The loss function to use while training, default to crossentropy\n",
    "      device: str\n",
    "        The device where to train end evaluate the neural network. It must be either cpu or a valid pytorch device (e.g. cuda). Default to cpu\n",
    "      \"\"\"\n",
    "      self.layers = []\n",
    "      self.optimizer = optimizer\n",
    "      self.net = net.to(device)\n",
    "      self.device = device\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "     return f\"{self.net}\"\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "  def train(self,\n",
    "            train_loader:'torch.utils.data.DataLoader',\n",
    "            validation_loader:'torch.utils.data.DataLoader',\n",
    "            learning_rate:'float'=.1,\n",
    "            epochs:'int'=10,\n",
    "            metrics:'dict[str,callable]' = {}) -> Tuple[dict[str,list[float]],dict[str,list[float]]]:\n",
    "    \"\"\"\n",
    "      A simple training loop for the neural network. It returns the epochs loss and accuracy history both on the training and the validation set. The tuple will be formatted as:\n",
    "      train loss, train accuracy, val loss, val accuracy\n",
    "      Parameters\n",
    "      ----------\n",
    "      train_loader: torch.utils.data.DataLoader\n",
    "        A dataloader containing the dataset that will be used for training the network\n",
    "      validation_loader: torch.utils.data.DataLoader\n",
    "        A dataloader containing the dataset that will be used for validate the network at the end of each epoch\n",
    "      learning_rate: float\n",
    "        The learning rate that will be used in the optimizer to train the network. Default to .1\n",
    "      epochs:\n",
    "        The number of training epochs, default to 10.\n",
    "    \"\"\"\n",
    "    net = self.net\n",
    "    optimizer = self.optimizer(net.parameters(), learning_rate)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    total_batch = int(len(train_loader.dataset) / train_loader.batch_size)\n",
    "    train_metrics_scores = {}\n",
    "    val_metrics_scores = {}\n",
    "    for key in metrics:\n",
    "        train_metrics_scores[key] = []\n",
    "        val_metrics_scores[key] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "\n",
    "            outputs = net(labels=labels, input_ids=inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            predicted_classes = torch.max(outputs.logits, dim=-1)[1].view(-1).cpu()\n",
    "            normal_labels = torch.max(labels, dim=-1)[1].view(-1).cpu()\n",
    "\n",
    "            stdout.write(f\"\\rbatch {batch_idx + 1}/{total_batch} ----- loss: {loss.cpu()} ----- {'-----'.join([f'{key}: {metrics[key](predicted_classes, normal_labels)}' for key in metrics.keys()])}\")\n",
    "            stdout.flush()\n",
    "\n",
    "        val_metrics, val_loss = self.__validate(validation_loader, metrics)\n",
    "        for key in metrics:\n",
    "          val_metrics_scores[key].append(val_metrics[key])\n",
    "        val_loss_history.append(val_loss)\n",
    "        train_metrics, train_loss = self.__validate(train_loader, metrics)\n",
    "        for key in metrics:\n",
    "          train_metrics_scores[key].append(train_metrics[key])\n",
    "        train_loss_history.append(train_loss)\n",
    "        out_str = \"======================================================================================================================================\\n\" + \\\n",
    "        f\"EPOCH {epoch + 1} training loss: {train_loss_history[-1]} - validation loss: {val_loss_history[-1]}\\n\" + \\\n",
    "        '\\n'.join([f\"EPOCH {epoch + 1} training {metric}: {train_metrics_scores[metric][-1]} - validation {metric}: {val_metrics_scores[metric][-1]}\" for metric in metrics.keys()]) + \\\n",
    "        \"\"\"\n",
    "======================================================================================================================================\n",
    "        \"\"\"\n",
    "        stdout.write(\"\\r\" + \" \" * len(out_str) + \"\\r\")\n",
    "        stdout.flush()\n",
    "        stdout.write(out_str)\n",
    "        stdout.flush()\n",
    "        print()\n",
    "    train_metrics_scores['loss'] = train_loss_history\n",
    "    val_metrics_scores['loss'] = val_loss_history\n",
    "    return train_metrics_scores, val_metrics_scores\n",
    "\n",
    "  def __validate(self, loader, metrics):\n",
    "    losses = []\n",
    "    metrics_scores = {}\n",
    "    for key in metrics.keys():\n",
    "      metrics_scores[key] = []\n",
    "    net = self.net\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader):\n",
    "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "\n",
    "            outputs = net(labels=labels, input_ids=inputs)\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss)\n",
    "            predicted_classes = torch.max(outputs.logits,  dim=-1)[1].view(-1)\n",
    "            for key in metrics.keys():\n",
    "              metrics_scores[key].append(metrics[key](predicted_classes.cpu(), torch.max(labels, len(labels.shape) - 1)[1].view(-1).cpu()))\n",
    "\n",
    "    average_loss = sum(losses)/(len(loader))\n",
    "    mean_metrics_scores = {}\n",
    "    for key in metrics.keys():\n",
    "      mean_metrics_scores[key] = sum(metrics_scores[key])/len(loader)\n",
    "    return mean_metrics_scores, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset:pd.DataFrame, tokenizer:BertTokenizer, columns:list[str]) -> pd.DataFrame:\n",
    "    return {column:\n",
    "        tokenizer(dataset[column], padding=True, truncation=True, return_tensors='pt')\n",
    "        for column in columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_c_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_c = NeuralNetwork(BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conclusion = tokenize_dataset(training_dataframe, bert_c_tokenizer, ['Conclusion'])['Conclusion']\n",
    "validation_conclusion = tokenize_dataset(validation_dataframe, bert_c_tokenizer, ['Conclusion'])['Conclusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c_dataset = Dataset(train_conclusion, train_y)\n",
    "validation_c_dataset = Dataset(validation_conclusion, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (/home/seppiabrilla/.var/app/com.visualstudio.code/cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|██████████| 2/2 [00:00<00:00, 197.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "data = datasets.load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/seppiabrilla/.var/app/com.visualstudio.code/cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-db438005facbdf53.arrow\n",
      "Loading cached processed dataset at /home/seppiabrilla/.var/app/com.visualstudio.code/cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-266920c1af0f6f34.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return bert_c_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = data.map(tokenize_function, batched=True)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================                                                                                                                                                                                                                             \n",
      "EPOCH 1 training loss: 7.13974142074585 - validation loss: 7.13974142074585\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                               \n",
      "EPOCH 2 training loss: 9.540169715881348 - validation loss: 9.540169715881348\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                             \n",
      "EPOCH 3 training loss: 1.36622154712677 - validation loss: 1.36622154712677\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                               \n",
      "EPOCH 4 training loss: 4.716699123382568 - validation loss: 4.716699123382568\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                             \n",
      "EPOCH 5 training loss: 1.57699453830719 - validation loss: 1.57699453830719\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                                 \n",
      "EPOCH 6 training loss: 1.9186840057373047 - validation loss: 1.9186840057373047\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                                 \n",
      "EPOCH 7 training loss: 1.8426910638809204 - validation loss: 1.8426910638809204\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                                   \n",
      "EPOCH 8 training loss: 0.16172224283218384 - validation loss: 0.16172224283218384\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                                 \n",
      "EPOCH 9 training loss: 0.6260102391242981 - validation loss: 0.6260102391242981\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n",
      "======================================================================================================================================                                                                                                                                                                                                                                    \n",
      "EPOCH 10 training loss: 0.19408947229385376 - validation loss: 0.19408947229385376\n",
      "\n",
      "======================================================================================================================================\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': [tensor(7.1397),\n",
       "   tensor(9.5402),\n",
       "   tensor(1.3662),\n",
       "   tensor(4.7167),\n",
       "   tensor(1.5770),\n",
       "   tensor(1.9187),\n",
       "   tensor(1.8427),\n",
       "   tensor(0.1617),\n",
       "   tensor(0.6260),\n",
       "   tensor(0.1941)]},\n",
       " {'loss': [tensor(7.1397),\n",
       "   tensor(9.5402),\n",
       "   tensor(1.3662),\n",
       "   tensor(4.7167),\n",
       "   tensor(1.5770),\n",
       "   tensor(1.9187),\n",
       "   tensor(1.8427),\n",
       "   tensor(0.1617),\n",
       "   tensor(0.6260),\n",
       "   tensor(0.1941)]})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_c.train(DataLoader(train_c_dataset), DataLoader(validation_c_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_pc_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_pc = NeuralNetwork(BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_pcs_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_pcs = NeuralNetwork(BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_pcs_numerical_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_pcs_numerical = NeuralNetwork(BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "**Do not mix models**. Each model has its own instructions.\n",
    "\n",
    "You are **free** to select the BERT-based model card from huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Examples\n",
    "\n",
    "```\n",
    "bert-base-uncased\n",
    "prajjwal1/bert-tiny\n",
    "distilbert-base-uncased\n",
    "roberta-base\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BERT w/ C\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bert_c.png\" alt=\"BERT w/ C\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BERT w/ CP\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bert_cp.png\" alt=\"BERT w/ CP\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BERT w/ CPS\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bert_cps.png\" alt=\"BERT w/ CPS\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Input concatenation\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/input_merging.png\" alt=\"Input merging\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "The **stance** input has to be encoded into a numerical format.\n",
    "\n",
    "You **should** use the same model instance to encode **premise** and **conclusion** inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 0.5 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using per-category binary F1-score.\n",
    "* Compute the average binary F1-score over all categories (macro F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "You start with individual predictions ($\\rightarrow$ samples).\n",
    "\n",
    "```\n",
    "Openess to change:    0 0 1 0 1 1 0 ...\n",
    "Self-enhancement:     1 0 0 0 1 0 1 ...\n",
    "Conversation:         0 0 0 1 1 0 1 ...\n",
    "Self-transcendence:   1 1 0 1 0 1 0 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You compute per-category binary F1-score.\n",
    "\n",
    "```\n",
    "Openess to change F1:    0.35\n",
    "Self-enhancement F1:     0.55\n",
    "Conversation F1:         0.80\n",
    "Self-transcendence F1:   0.21\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You then average per-category scores.\n",
    "```\n",
    "Average F1: ~0.48\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate **all** defined models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Compute metrics on the validation set.\n",
    "* Report **per-category** and **macro** F1-score for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Compare** classification performance of BERT-based models with respect to baselines.\n",
    "* Discuss **difference in prediction** between the best performing BERT-based model and its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "You can check the [original paper](https://aclanthology.org/2022.acl-long.306/) for suggestions on how to perform comparisons (e.g., plots, tables, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model card\n",
    "\n",
    "You are **free** to choose the BERT-base model card you like from huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Training\n",
    "\n",
    "You are **free** to choose training hyper-parameters for BERT-based models (e.g., number of epochs, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to address the assignment (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
